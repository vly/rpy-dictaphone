\documentclass[11pt,a4paper,titlepage]{report}


% Document settings

\title{OSP Portfolio \\ Team $\langle$sql injection$\rangle$}

\author{
  Neil Ang\\
  \texttt{s3251533}
  \and
  ``Alfred" Yang Yuan\\
  \texttt{s3363619}
  \and
  Val Lyashov\\
  \texttt{s3366222}
}

\date{Semester 2, 2013}


% Change section numbering
%\renewcommand\thesection{\Roman{section}}
%\renewcommand\thesubsection{\Alph{subsection}}
\renewcommand\thesection{\arabic{section}}
\renewcommand\thesubsection{\thesection.\arabic{subsection}}


% Enable smart quotes
\usepackage [english]{babel}
\usepackage [autostyle]{csquotes}
\MakeOuterQuote{"}

% Alias pi name
\usepackage{xspace}
\newcommand{\rpi}{\textit{Raspberry Pi\textsuperscript{\textregistered}}}
\newcommand{\rpis}{\textit{Raspberry Pi\textsuperscript{\textregistered}s}}

% Side by side graphics
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

% Switch to biblatex
\usepackage{biblatex}
\bibliography{computer-vision}
\bibliography{audio}
\bibliography{servo}

% Add the bib to the toc
\DefineBibliographyStrings{english}{
  bibliography = {Bibliography},
}

% Better table height
\usepackage{tabu}

% The appendix
\usepackage{appendix}

% Code highlighting
\usepackage{listings}
\lstset{basicstyle=\ttfamily}

% For marking what's left to do
\usepackage{color}


\begin{document}


\maketitle

\pagebreak
\tableofcontents
\thispagestyle{empty}
\pagebreak

\section{Introduction}

Live presentations audio recording is a well known problem for organisers, speakers and attendees alike. While there are significant amount of solutions available, factors such as cost, complexity and portability often limit widespread adoption. While existing services such as Echo360 and Lectopia work very well in recoding in a lecture theatre environment, ad-hoc deployment in smaller venues can be costly and logistically difficult to achieve.
 
In this project we present a cost-effective, light-weight presentation recording solution that requires little setup time. Designed with flexibility and rapid ad-hoc deployment as the guiding imperatives, the solution presents a good alternative for both personal and organisational use.


\section{Goals and Objectives}


The focus of the project has been to deliver an easy-to-use, fast to setup, and cost-effective solution to live presentation recording. The following table provides an overview of the core goals and projective we have strived towards over the course of the product development. The priority figure is the value on a scale of 1 to 5 (low to high respectively) that the design team have placed on the goal/objective (particularly during the initial prototype phase).


\begin{center}
\begin{table}
\begin{tabular}{|l|p{6cm}|c|}
    \hline
    \textbf{Goal / Objective} & \textbf{Description} & \textbf{Priority} \\ \hline
    Cost-effectiveness & One of the main factors that determines the viability of the end-product is the overall cost effectiveness. The aim is to provide a sub-\$200 product in a self-assembly state. & 5  \\ \hline
    
    Ease of use & The product has to be easy to use and operate by end-users. This includes setup, operation, troubleshooting stages of use. Some technical knowledge would be a required for the current form of the product. & 4  \\ \hline
    
    Semi-portability & As the project aims to primarily cater for ad-hoc recording scenarios, portability plays an important part of the overall system design. This takes form in complexity of assembly, size and weight of the finished product, the power usage and requirement, as well as the storage capacity of the recording functionality.& 3  \\ \hline

    Off-the-shelf hardware & To make the final product accessible to the largest share of prospective audience, the availability of compontents that make up the product becomes critical for the commercialisation of the project.& 2   \\ \hline

    Modularity & Partially related to the off-the-shelf hardware objective, modularity of components and accessories would ensure greater usage and types of operating environments the product can be deployed in.& 3  \\ \hline



\end{tabular}
\end{table}
\end{center}

\section{Feedback and Self-Reflection}
\subsection{Self Assessment}
\subsection{Summary of Prototype Demonstration}
\subsection{Peer Review Summary}

\textcolor{red}{This is waiting on the lab instructor to give us back our peer-review sheets.}


\subsection{Self Reflection / Lessons Learned}

\textcolor{red}{Individual responses here...}


\subsection{Description of how each learning objective is addressed}


Over the course of the project development, this team have made significant leaps in their knowledge of computer-hardware interfacing, operating system level functionality and application optimisation in a resource constrained environment. This section highlights how various learning objectives of the OSP course have been met.
 
\subsubsection{Learning Objective 1}
Over the course of the project, all of the ongoing development code has been managed through Github to ensure team members were always using the latest stable code when working on their assigned tasks.

With the face detection (computer vision) generating major CPU load when running on the Raspberry Pi, the team has gone through a significant amount of iterations of the functionality in order to achieve real-time response to the enviroment.


For efficiency during the initial development stages, most hardware interface interaction (e.g. servo controller, PIR sensor etc.) were prototyped using the Python programming language, and later ported into C when performance optimisation was required.


Debugging and performance has been used extensively over the course of the project for both hardware and software components. For Python, Pudb was the debugger of choice. For C/C++ Instruments and LLDB/GDB were utilised. On the hardware side, servo controller built-in error-checking was utilised for all communication to ensure hardware errors were caught and dealt with.
 


\section{Assumptions and Dependencies}
\section{General Constraints}
\section{Development Methodology}

\textcolor{red}{Reinforce how we met learning objective 1 here.}


\subsection{Programming languages}

\textcolor{red}{Python for prototyping, C/C++ for performance.}


\subsection{Development tools}

\subsection{Collaboration tools}

There were three main collaboration tools we used to progress this project: GitHub, Email and face-to-face meetings.

\subsubsection{GitHub}

From the start we setup a private\footnote{Access to this repository is available upon request.} git repository hosted on GitHub to store all our experimentation, development and production code. Since our project specification and portfolio were typeset in \LaTeX, we were also able to version control that as well. For all other project documentation (such as development logs and bibliography) we took advantage of GitHub's built in wiki to store this information.

GitHub worked for us because git was a version control system we all wanted to use, it was a tool we were familiar with, and it made it easy to identify when a piece of work had been completed.

\subsubsection{Email}

As postgraduate students, we all had competing priorities with our time which meant that we could not work together in person regularly. It was also difficult to meet due to the amount of hardware involved. As a result we often worked in isolation with very defined tasks.

So for us email was key for effective communication and delegating tasks. Once a week we would a group message was used to talk about our progress and goals for the coming week.

\subsubsection{Face-to-face meetings}

Being all together at once was a rare occasion and we had only five out-of-class-hours group meetings that we all attended. We used it to visually demonstrate what we had achieved with our tasks and discuss the issues arising from it. 

If another group member was assigned to take over a task (e.g. handing over code to be optimised or integrated), we used this time to instruct in detail how the prototype code worked or how the hardware had been configured. 

\section{Difficulties Encountered}
\section{Architecture}
\subsection{System Design including configuration}
\subsection{Data Design}
\subsection{Program Design}


The software architecture for this project is basically a C/S (Client and Service) pattern. The server and client are connecting with Linux socket, using UDP protocol. The project has been divided into separated components, and after we build demo program for each part, we calculate the CPU usage, and than distribute the component into server and client.In terms of components, they adopt messages to communicating with each other. That is to say, each component only cares about their own jobs, which improves the hoisin and independency for the system.

\textcolor{red}{TODO: Add communication digram here}


\subsubsection{Architecture Decisions}

\begin{center}
\begin{table}
\begin{tabular}{|p{0.3\textwidth}|p{0.2\textwidth}|p{0.5\textwidth}|}
    \hline
    \textbf{Aspects} & \textbf{Options} & \textbf{Decisions} \\ \hline
    
     Communication & 1. TCP \newline 2. UDP & 1. This is a small project that we only measure two PI communications, so the package loses shall happen in low possibility. \newline  2. In terms of complexity of communication, we do not need the message queue manages whether the message is delivered or not. \newline \textit{Option 2 adopted.} \\ \hline

     Performance & 1. Idle \newline 2. Multiple threads & 1. There are brunches of components needs to wait communication messages to take next step. If we use idle function, it shall increase the queries between each component, dropping the system's cohesion. \newline  2. The worst case for face detection algorithm shall take a long time on detecting, dropping the system’s performance dramatically. \newline \textit{Option 2 adopted.} \\ \hline

     Library & 1. Open CV \newline 2. PI face detection library & 1. Easy to debug on the laptop. \newline  2. Open CV library supports the USB camera, which can easily located on servo. \newline \textit{Option 1 adopted.} \\ \hline
     
     Thread Communication & 1. Pipeline \newline 2. Sharing memory & 1. Sharing memory provides better performance on communication. \newline  2. There are too many components that need to communication, if we use pipeline, it shall increase the complex for the project and decrease the maintenance. \newline \textit{Option 2 adopted.} \\ \hline
     
     Memory & 1. Instance every copy \newline 2. Copy on write & 1. All components may assess the message data. if every component create an instance, there shall be a lot memory waste for brunches of different instance for same object. \newline  2. Copy on write provides more performance; because it shall not spend too much time on copying the data form one object to another. \newline \textit{Option 2 adopted.} \\ \hline

     Message scheduling & 1. FCFS \newline 2. More complex algorithms & 1. FCFS is easy to implement and maintain. \newline  2. This is an easy project, and every message may have same priority. \newline \textit{Option 1 adopted.} \\ \hline

\end{tabular}
\end{table}
\end{center}


\textcolor{red}{TODO: Add sequence digram here}

\subsubsection{Sharing memory for threads communication}

All the threads have their own stack, but they shall share the heap memory. So we build all the service in heap memory. (use malloc) In addition,  in order for easy accessing from different threads, we allocate a pointer in static memory block that point to the heap. 

The code of Singleton is shown as below:

\textcolor{red}{TODO: INSERT CODE}


\subsubsection{Memory Scheduling}

The most different part for the message definition is that the length of the message does not know before it construct. However, we cannot create a big buffer because of the memory limited for the PI. As a result of that, I use a strategy only define 1 char array, and use malloc to increase the length dynamically.The code of dynamical message length is shown as below:

\textcolor{red}{TODO: INSERT CODE}


\subsubsection{Copy on write}

PI has little memory size, so it is impossible to reallco memory for passing large frame data buffer that come from face detection frame. More specify, it turn to be waste of memory coping more than one instance of any kind of instance in the project. In order to reduce the usage of memory, we adopt the Copy on write strategy. This strategy only makes a copy when there is some modification on the object, and the most of time they shall only create a new pointer point to the original address.The trade-off for this strategy is that it is no thread-safe, because more than one thread want to access the object when about to change the original data. However, in our project, all the processes, such as face detection and messages, are read-only process, which means that we do not have to consider race condition when we use this approach. The code of message is shown as below:


\textcolor{red}{TODO: INSERT CODE}

\subsubsection{Message serialization}

As the communication between server and client, we encode the message data into an array that uses '\textbackslash0' at the end. And the message array can also be encoded into message.The code of message serialization is shown as below:

\textcolor{red}{TODO: INSERT CODE}


\subsubsection{Message queue class}

The message queue acts as the communication component in the system. All other components push message to the message queue and the dispatch messages to appropriate component. The most important functionality for message queue should be synchronization. That is to say, only one thread can push or pop message at the same time in order to prevent race condition. More specific, the message queue use conditional valuable to control the race condition. The core code of message queue is shown as below:

\textcolor{red}{TODO: INSERT CODE}


\subsubsection{FCFS strategy}

The code shows that we use the simplest strategy to scheduling the message, which means that when there is a message push into the message queue, the dispatching thread shall read the first message in the message queue send to the appropriate component to serve it. In addition, the dispatch thread shall be hinge up and wait when there are no messages in message queue.The message shall be dispatched messages using design pattern: chain of responsibility. The process base on the message ID, which have been introduced in message protocol part. The dispatch component shall query message id and push the message to the serve components.The core code of client message dispatching is shown as below:

\textcolor{red}{TODO: INSERT CODE}





\subsubsection{Software Design}

\subsubsection{Source code or patches for all original work.}


Source code for simulating face detection. \textcolor{red}{TODO: move this under testing heading?}

\begin{lstlisting}[frame=single]
#include <iostream>
#include <cstdlib>
#include <unistd.h>
#include <stdio.h>
#include <string.h>

#define NA_DEFAULT_WAIT 3
#define NA_MIN_TOLERANCE 2
#define NA_DEFAULT_TOLERANCE 5

#define MAX(X,Y) ((X) > (Y) ? (X) : (Y))

int rand_num()
{
    return (rand() % 200 + 1) - 100;
}

int rand_with_tolerance(int t, int l)
{
    setvbuf(stdout, NULL, _IONBF, 0);
    int i;
    do {
        i = rand_num();
    } while (i > l+t || i < l-t);
    return i;
}

int main(int argc, const char * argv[])
{
    int x = 0;
    int y = 0;

    int t = NA_DEFAULT_TOLERANCE;
    int s = NA_DEFAULT_WAIT;

    for (int i = 1; i < argc; i++) {
        int v = i+1 <= argc;
        if (strcmp(argv[i], "-t") == 0 && v) {
            t = std::stoi(argv[i+1]);
            t = MAX(t, NA_MIN_TOLERANCE);
        }
        else if (strcmp(argv[i], "-s") == 0 && v) {
            s = std::stoi(argv[i+1]);
            s = MAX(s, 0);
        }
        else if (strcmp(argv[i], "-h") == 0) {
            std::cout << "Usage: -t <tolerance>
             -s <seconds>" << std::endl;
            return EXIT_SUCCESS;
        }

    }

    while (true) {
        x = rand_with_tolerance(t, x);
        y = rand_with_tolerance(t, y);
        printf("%d,%d\n", x, y);
        sleep(s);
    }

    return EXIT_SUCCESS;
}
\end{lstlisting}



\section{Testing Issues}
\subsection{Testing Done}
\subsection{Performance Bounds}
\subsection{Performance Experiments}

\section{Roles and Responsibilities}

As outlined in our original project specification, there was overlap in our teams individual strengths so the roles were divided with some shared responsibilities. Although at different levels, we wanted all three group members to be involved with experimenting with the hardware, writing software and documenting. The cross-over in roles made the progress slightly slower, but allowed each of us to trial something new.

\subsection{Val Lyashov}

\textcolor{red}{Add table about hardware solution}

\subsection{"Alfred" Yang Yuan}

\textcolor{red}{Add table about software solution}

\subsection{Neil Ang}

\textcolor{red}{Add table about whatever is left}


\section{Breakdown of Work Done by Team Member}

Each team member kept a detailed log of their activities. Below is a summary of what they achieved each week.

\subsection{Val Lyashov}

\begin{description}

  \item[Week 2] \hfill \\
      Formed team with Neil and discussed ideas for the project.
  \item[Week 3] \hfill \\
      Voted for a team restructure.
  \item[Week 4] \hfill \\
      Acquired new team member.
  \item[Week 5] \hfill \\
      Demonstrated cross-compile milestone in lab.
  \item[Week 6] \hfill \\
      ...
  \item[Week 7] \hfill \\
      ...
  \item[Week 8] \hfill \\
      Took a trip to Bunnings with Neil to build the servo mount.
  \item[Week 9] \hfill \\
      ...
  \item[Week 10] \hfill \\
      Presented project to lab group (see appendix). Started work on final project portfolio submission.
  \item[Week 11] \hfill \\
      Worked on final project portfolio submission.
  \item[Week 12] \hfill \\
      Continued work on project portfolio submission.

\end{description}


\subsection{"Alfred" Yang Yuan}

\begin{description}

  \item[Week 4] \hfill \\
      Joined the team.
  \item[Week 5] \hfill \\
      Demonstrated cross-compile milestone in lab.
  \item[Week 6] \hfill \\
      ...
  \item[Week 7] \hfill \\
      ...
  \item[Week 8] \hfill \\
      ...
  \item[Week 9] \hfill \\
      ...
  \item[Week 10] \hfill \\
      Presented project to lab group (see appendix). Started work on final project portfolio submission.
  \item[Week 11] \hfill \\
      Worked on final project portfolio submission.
  \item[Week 12] \hfill \\
      Continued work on project portfolio submission.

\end{description}

\subsection{Neil Ang}
See appendix for daily log of tasks.
\begin{description}

  \item[Week 2] \hfill \\
      Formed team with Val and discussed ideas for the project. Purchased \rpi. Completed cross-compile milestone. Installed Arch linux on \rpi. Started research into computer vision libraries.
  \item[Week 3] \hfill \\
      Purchased a RPi camera board. Installed Raspbian on \rpi. Researched depth sensing on the device. Prototyped face detection code on MBP with OpenCV. Acquired a web camera and tested on the device. Voted for a team restructure.
  \item[Week 4] \hfill \\
      Acquired new team member. Continued experimenting with OpenCV to improve performance running on the device. Started work on project specification.
  \item[Week 5] \hfill \\
      Demonstrated cross-compile milestone in lab. Continued work on project specification. Wrote test program to simulate face detection for Val.
  \item[Week 6] \hfill \\
      Finished the project specification. Ported the PIR motion sensing Python code to C.
  \item[Week 7] \hfill \\
      Researched IPC techniques. Acquired the servos and started work on porting code to C++.
  \item[Week 8] \hfill \\
      Solved issues with servo powers. Finished work on C++ wrapper for servos. Got face detection based movement working. Took a trip to Bunnings with Val to build the servo mount.
  \item[Week 9] \hfill \\
      Debugged issues with the servo wrapper when running on the Pi. Wrote simple daemon for automatically starting the server/client code. Wrote presentation and speech for next milestone.
  \item[Week 10] \hfill \\
      Presented project to lab group (see appendix). Started work on final project portfolio submission.
  \item[Week 11] \hfill \\
      Worked on final project portfolio submission.
  \item[Week 12] \hfill \\
      Continued work on project portfolio submission.

\end{description}


\section{Summary and Conclusions}
\section{References}



\begin{appendices}

\chapter{Gantt Chart}

\includegraphics[width=\textwidth]{graphs/gantt-chart.pdf}


\chapter{Project Specification }

\textcolor{red}{Add project spec}

\chapter{Presentation speech (Milestone 3)}


Hi, we're team $\langle$sql injection$\rangle$. We are Alfred, Val and Neil.Because the raspberry pi is small and portable, we wanted to build something that would take advantage of this. We also liked the idea of building something that would respond to its surroundings. So we decided on an ambitious project to modernise the phonograph (commonly known as a dictaphone), but combining subject tracking with targeted audio recording equipment.

Here you can see Alfred, demoing the project. As he moves the left/right/up/down the Pi detects his new position and reorients the the shotgun microphone. This particular microphone is designed for targeted audio, and will record sound up to 3 metres while excluding background noise. The intended use of this product would be for recording lectures and tutorials, and producing podcasts. Our original design also included audio-to-text conversion, so we could generate transcripts, but the translation library we were using didn’t work well with Australian or Russian or Chinese accents... so... we dropped that part.

The first learning objective was about design, development and debugging a complex program on the Pi. With three group members with different backgrounds, we had different preferences for building the project. Our approach was to modularise the solution and allow multiple members work on the same thing but in different languages. For example, the servo code was quickly prototyped in python to ensure it functioned correctly. Once it was working, another group member re-wrote the python into C++ code. As most of you would have come up against, compiling on the Pi was slow. So a lot of early development was done on laptops then later tested on the device. There were a few circumstances where code would work on our machines and not the device, but that was due to the differing versions of g++ we were using and not a big issue to fix. A bonus of working this way, was that we had access to IDE debuggers and static analysers. While trying to improve the performance of our code, we ran the program through "Instruments", which detected where the code was the slowest and also picked up on a memory leak. Since our project was about interacting with the physical world (i.e detecting faces, moving motors, sensing motion). We had to run a lot of manual benchmarking. Basically we would tweak the degree of movement in servos, re-run the code and roughly evaluate if it was getting better or worse.Finally, all source code was checked into git, and hosted on GitHub. We heart GitHub. We used it for our source code, project specification and to record our development logs and bibliography.

Leaning objective 2 was about assessing trade-offs in hardware on a constrained system. To make things easier we put decided to split the workload over two Pis. One for face detection and movement, the other for audio recording and processing. For the subject tracking we looked at multiple solutions. We first thought about using depth sensing with something like a MS Kinect sensor, but found out that it was too resource intensive, so we were limited to 2D visual processing, and settled on face detection because of the suitability for the solution. We could also have done colour tracking or background masking.To perform real-time face detection you take a snapshot from the camera, convert it to grayscale, equalise the histogram, then use a trained classifier (in our case Haar-like features) to scan the image for face shapes at different scales, repeat.There is a C++ based open source library called OpenCV, which implements the classifier we wanted to use. It also supports GPU processing, but only NVIDIA GPUs (which the Pi doesn’t have). So to run it on the Pi we had to do the whole thing on CPU. This gave us the challenge of finding ways to optimise the CPU-based face detection so that it would run in real-time and actually detect faces. We took an iterative approach to this.So we started a basic face detection script and ran it. It used 100\% of the CPU and took about a 15 seconds to process each frame. We were using a 720p camera, so the first obvious step was to reduce the frame size so we were processing to less. At 320x240 we able to process a frame every 4-5 seconds and dropped CPU usage down to 70\%. We also added some limits on how it searched, by giving it a minimum face size, and set it to only perform a rough search for the biggest face. We got it at around 50\% CPU and 2-3 seconds per frame. Not bad.Alfred, then had some brilliant ideas on improve the performance further. Original script was creating a matrix of pixels that were copied into RAM and processed, he switched the code to use the pointers from the cameras instead of rebuilding the pixel matrix locally, and processed them on background threads. When the camera moves the frames that are still being processed are dropped, because the position has changed. Also, because of the way we predict the program would be used, we could reduce the pixels to process even further, by remembering the last face position and targeting just that area first. With all the additions we got the face detection and movement happening at an acceptable speed. There are some limitation to the implementation, such as moving too fast for the camera, or recording a profile shot - but we chalk this up to the constraints of using a raspberry pi. We ran the same scripts on a modern day MBP, and the accuracy and speed were at least 4 times better.A face is a complex shape, so we also thought about detecting just an eye or a nose. We ran some  tests, which improved the overall FPS, but the accuracy was a lot less. So full face detection was our best option. 

Also in attempting to improve the overall performance of project modules (eg face detection, real time recording). We investigated modification to process scheduling, however due to the bare bones nature of our system, there was no competition for CPU time.

Learning objective 4 was about team work. This project was particularly hard to work on as a team. Like other teams in this class would have come across, we faced with the obstacle of limited time for such an ambitious project. Two of our members work full-time, and the other studies full-time, so we had to do a lot of work at night in isolation. The obvious solution was to modularise the project into components, and have each member deliver each week. As can be seen in the table, we also tried to reduce dependencies on deliverables, so if one ran over time it wouldn’t impact the next weeks deliverables.But our biggest team issue with the amount of hardware we were using. Besides the Pis, we only had purchased one microphone, one motion sensor, one set of servos etc. So only one team member had physical access to a piece of hardware at a time. To overcome this obstacle, we came up with the idea of writing hardware simulation scripts. For example, we wrote a C++ program that would simulate face detection and output fake coordinates - This meant Val could fine tune the motor movement without having the physical camera or the face detection code complete.Another problem with the amount of hardware, was that when we did meet together, we needed to bring a lot of equipment. The photo on the slide doesn’t really do justice to what it was like to haul 3x laptops, 3x pis, a router, networking cables, servo cables, PIR sensor breadboards, plus tools etc.Finally, as a team we also wanted to do cross-skill development, which made things slower but increased our personal learning. Each members primary role was designed to make use of their best strengths. But we also included cross-overs in the task so we could each try something new. For example, although Val was our expert hardware guy, Neil (who had very limited hardware experience) got to write some of the C interfaces to the hardware, which is something he had never done before. We did similar things with all roles so that we each learned something new.

\end{appendices}

\nocite{*}
\printbibliography[heading=bibintoc]


\end{document}