\documentclass[11pt,a4paper,titlepage]{report}


% Document settings

\title{OSP Portfolio \\ Team $\langle$sql injection$\rangle$}

\author{
  Neil Ang\\
  \texttt{s3251533}
  \and
  ``Alfred" Yang Yuan\\
  \texttt{s3363619}
  \and
  Val Lyashov\\
  \texttt{s3366222}
}

\date{Semester 2, 2013}

% Set watermark while in development
%\usepackage{draftwatermark}
%\SetWatermarkText{DRAFT}
%\SetWatermarkScale{5}
%\SetWatermarkColor[gray]{0.95}


% Change section numbering
%\renewcommand\thesection{\Roman{section}}
%\renewcommand\thesubsection{\Alph{subsection}}
\renewcommand\thesection{\arabic{section}}
\renewcommand\thesubsection{\thesection.\arabic{subsection}}


% Enable smart quotes
\usepackage [english]{babel}
\usepackage [autostyle]{csquotes}
\MakeOuterQuote{"}

% Alias pi name
\usepackage{xspace}
\newcommand{\rpi}{\textit{Raspberry Pi\textsuperscript{\textregistered}}}
\newcommand{\rpis}{\textit{Raspberry Pi\textsuperscript{\textregistered}s}}

% Side by side graphics
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

% Switch to biblatex
\usepackage{biblatex}
\bibliography{computer-vision}
\bibliography{audio}
\bibliography{servo}

% Add the bib to the toc
\DefineBibliographyStrings{english}{
  bibliography = {Bibliography},
}

% Better table height
\usepackage{tabu}

% The appendix
\usepackage{appendix}
\usepackage{pdfpages}


% Code highlighting
\usepackage{listings}

\lstset{
    language=C++,
    basicstyle=\small\sffamily,
    numbers=left,
    numberstyle=\tiny,
    frame=tb,
    columns=fullflexible,
    showstringspaces=false
}


% For links
\usepackage{hyperref}



% For marking what's left to do
\usepackage{color}



%For process listing
\usepackage{dirtree}
\usepackage{multicol}

\begin{document}


\maketitle

\pagebreak

\includepdf[fitpaper=true,pages=-]{reflections/neil.pdf}
\includepdf[fitpaper=true,pages=-]{reflections/val.pdf}
\includepdf[fitpaper=true,pages=-]{reflections/alfred.pdf}


\tableofcontents
\thispagestyle{empty}
\pagebreak

\section{Introduction}

Live presentations audio recording is a well known problem for organisers, speakers and attendees alike. While there are significant amount of solutions available, factors such as cost, complexity and portability often limit widespread adoption. While existing services such as Echo360 and Lectopia work very well in recoding in a lecture theatre environment, ad-hoc deployment in smaller venues can be costly and logistically difficult to achieve.
 
In this project we present a cost-effective, light-weight presentation recording solution that requires little setup time. Designed with flexibility and rapid ad-hoc deployment as the guiding imperatives, the solution presents a good alternative for both personal and organisational use.


\section{Goals and Objectives}


The focus of the project has been to deliver an easy-to-use, fast to setup, and cost-effective solution to live presentation recording. The following summary provides an overview of the core goals and projective we have strived towards over the course of the product development. The priority figure is the value on a scale of 1 to 5 (low to high respectively) that the team have placed on the goal/objective (particularly during the initial prototype phase).


\begin{description}

  \item[Cost-effectiveness (Priority 5)] \hfill \\
      One of the main factors that determines the viability of the end-product is the overall cost effectiveness. The aim is to provide a sub-\$200 product in a self-assembly state.
  \item[Ease of use (Priority 4)] \hfill \\
      The product has to be easy to use and operate by end-users. This includes setup, operation, troubleshooting stages of use. Some technical knowledge would be a required for the current form of the product.
  \item[Semi-portability (Priority 3)] \hfill \\
      As the project aims to primarily cater for ad-hoc recording scenarios, portability plays an important part of the overall system design. This takes form in complexity of assembly, size and weight of the finished product, the power usage and requirement, as well as the storage capacity of the recording functionality.
  \item[Off-the-shelf hardware (Priority 2)] \hfill \\
      To make the final product accessible to the largest share of prospective audience, the availability of components that make up the product becomes critical for the commercialisation of the project.
  \item[Modularity (Priority 3)] \hfill \\
      Partially related to the off-the-shelf hardware objective, modularity of components and accessories would ensure greater usage and types of operating environments the product can be deployed in.

\end{description}



\section{Feedback and Self-Reflection}

We were fortunate to have formed a very focused team with complimentary skill sets. The feedback we received was very much a reflection of our hard work and determination. Overall we are very proud of what we have achieved.


\subsection{Self Assessment}

From the start we knew this would be an ambitious project. We wanted to build something impressive, useful and which would overly meet the learning objectives. On reflection we did over-estimated what we could achieve in the time frame, however the positive feedback from what we did accomplish have made us still regard this project as a success.

Each team member brought a vital skill set to the project. Val's hardware and unix knowledge gave us the building blocks for creating the project, Neil's programming skills created working prototypes for each component, and Alfred's optimisation ability polished the project and brought all the pieces together. Although we needed these skills to develop the solution, it was the hard working nature of all of the team members that was the real backbone of the project.



During the lab demonstration we were asked the question "What mark do you think you deserve?" Our response was "HD" and we still stand by this answer. We treated all aspects of the project seriously and put in the hours needed to achieve the end solution. We delivered each milestone on time and didn't compromise on the quality of our work.


At the end of this project there are a number of design changes we would make during the production of a second prototype. These include:

\begin{itemize}
    \item \textbf{Stronger servos:} The servos used for the project, although worked OK supporting the weight of the usb camera, struggled with the extra weight introduced by the shotgun microphone, cables, component mount and clamps. Spending approximately an additional 20% more money would provide better servos which come standard with metal gear kits.

    \item \textbf{Mounts and clamps:} The team have gone through various versions of camera and microphone mount prototypes before ending up with the final one used during the presentation. Although solid and fully adjustable, weight was a major tradeoff. The central shaft also reduced the vertical movement by approximately 30 degrees. A dedicated pan/swing mounting assembly as those used for dslr/movie camera mounts would provide greater range and smoothness of movements.

    \item \textbf{Camera:} Although the USB camera used for the project ended up working quite well, the resource constraints of the Raspberry Pi have forced the team to squeeze efficiency for face detection to work in real-time. These included significantly reducing the frame capture size and number of features detected. A camera with greater hardware-level control over the output (e.g. monochrome capture mode) would help to further reduce the resource overhead. The native Raspberry Pi camera module seams like a good candidate, however, the current drivers and documentation are not mature enough for achieving the required results in this instance.

    \item \textbf{Power source:} Having gone through a number of issues of getting a consistent external power source to drive the servos via the USB servo controller, and trying to manage the cable spaghetti caused by all the components a single power source to drive all the parts of this project would go a long way to resolve these issues. External laptop battery packs, although fairly heavy, do provide consistent power to run the project for at least 6 hours. The added bonus, is the size of such a battery pack could also serve as a base for the servo assembly.
\end{itemize}


\subsection{Summary of Prototype Demonstration}

Due to the hard work of all team members we were able to demonstrate a working prototype in our Week 10 laboratory. The presented product included an auto-startup feature so no laptop or router were needed for setup. We were able to successfully demonstrate to the class our face tracking, audio recording and motion sensing components of the solution. 

Although what we presented was a cutdown version of what was proposed in the project specification (missing the speech-to-text translation), we were very proud of what we had achieved and the positive reception the project received from our peers.

In preparation for the  demonstration we developed a short slide show, written speech and summary handout. All of these are available in the appendix. We also created a video of the working product incase we encountered technical issues during live demo. 

\begin{center}
\href{http://www.youtube.com/watch?v=hckEBpT1VcU}{
    \includegraphics[width=\textwidth]{graphs/youtube.png}
}
\end{center}
See the video here: \url{http://www.youtube.com/watch?v=hckEBpT1VcU}



\subsection{Peer Review Summary}

The reviews from our classmates during the presentation were very positive. There have been encouragement from a number of students and our lab assistant to continue development of this project towards a commercially viable state. Of the 23 peer review forms we received, no one suggested a grade lower than a High Distinction, which was also very encouraging. 

Some suggestions offered by the peer feedback forms as well as verbally included:

\begin{itemize}
    \item demonstrate the inter workings of the \rpis\xspace (a behind-the-scenes view of sorts) through a streaming debug activity to an external monitor
    \item debug audio monitoring capability
    \item move to a better spec ARM board such as the Beagleboard to improve the performance of face detection
\end{itemize}



\subsection{Self Reflection / Lessons Learned}

Although we had a talented group, each individual team member took something away from this project. Below are reflections from each group member in their own words.

\subsubsection{Val}

Having never worked with a Raspberry Pi before, I found the project quite enlightening, particularly in understanding the lower-level hardware interfacing through GPIO pins. 

The project also provided an awesome opportunity for me to learn off the other team members concepts and techniques I would otherwise encounter only as theory. It's been great to have such a wide knowledge base to draw upon when working towards troubleshooting and resolving hardware and software issues.

The work involving the servos and the Pololu servo controller was also the first time I had a chance to prototype a hardware device library from API documentation, which has subsequently greatly increased my confidence in own abilities.

Lastly, being required to work on a resource constrained device (Raspberry Pi) has really highlighted the value of measurement and optimisation of system performance, which provided real-world context a lot of the topics covered during our lectures.

\subsubsection{Alfred}

During the lab, I learnt how to use linux command, and how to use cross-complier to build a linux image.

I used a lot multi-threading programming, as a result of that I am now quiet familiar with Synchronisation in a linux way. In addition, the face detection had requirement on memory management, so I spent a long time to investigate how to implement this in C++. Also, the message dispatch helped me understand about how to schedule jobs.

Finally, thanks for Neil and Val, they help me a lot in this project. This is definitely the most successful group work for me while at RMIT.


\subsubsection{Neil}

This project gave me an excellent opportunity to try a lot of new things. Working on the \rpi\xspace and building a subject tracking system was something that I had never done before.


I had also never written a interface to device hardware before. It was very hard to debug but very satisfying once it was working. This also gave me the opportunity to expand my C and C++ experience.

The face detection was also very fun to work with. It was surprising easy to program and it has given me ideas for other potential projects where I could use this.


As a team I think we functioned very well. I normally try to avoid teamwork but our collection of skills and determination meant we delivered a project of a high standard.


Finally, this project also gave me a chance to create two new open source projects. While researching how to interact with our hardware I found very little code examples available. In an effort to help others, I decided to open source hardware based components of the project.


\subsection{Description of how each learning objective is addressed}


Over the course of the project development, this team have made significant leaps in their knowledge of computer-hardware interfacing, operating system level functionality and application optimisation in a resource constrained environment. This section highlights how various learning objectives of the OSP course have been met.
 
\subsubsection{Learning Objective 1}
For the duration of this project, all of our ongoing development code has been managed through Github to ensure team members were always using the latest stable code when working on their assigned tasks.

With the face detection (computer vision) generating major CPU load when running on the Raspberry Pi, the team has gone through a significant amount of iterations of the functionality in order to achieve real-time response to the environment.


For efficiency during the initial development stages, most hardware interface interaction (e.g. servo controller, PIR sensor etc.) were prototyped using the Python programming language, and later ported into C when performance optimisation was required.


Debugging and performance has been used extensively over the course of the project for both hardware and software components. For Python, Pudb was the debugger of choice. For C/C++ Instruments and LLDB/GDB were utilised. On the hardware side, servo controller built-in error-checking was utilised for all communication to ensure hardware errors were caught and dealt with.

A positive outcome from this project was that two original pieces of work we created for the solution have now been open sourced so other people building similar projects can leverage our work. We open sourced our code for interacting with a PIR-sensor\footnote{\url{https://github.com/neilang/pir-monitor}} on the \rpi. We also open sourced our C++ wrapper\footnote{\url{https://github.com/neilang/maestro-wrapper}} for interacting with the Polulu maestro servo controller. We selected these pieces of code because during our research we could not find any adequate existing examples for this.



 
\subsubsection{Learning Objective 2}

Working with the Raspberry Pi, our team encountered significant challenges in producing a functional project prototype while having so little processing power available at our disposal.

Significant amount of trial and error was carried out in optimising performance of face tracking functionality of the our project (as can be observed by the Github repository history). 

The target(s) coordinates derived from face-tracking functionality were then used to tilt and pan the assembly via two servos at the base of the camera and microphone combo. Initial experimentation with control of servos via Raspberry Pi's GPIO pins shown to be too inaccurate and resulting servo movement too jagged to be used due to CPU interrupts effecting the clock cycle count. The chosen solution was to acquire an external hardware servo controller, the Pololu Maestro. Running into issues with available libraries; our team put together a control library to communicate with the controller over the virtual  serial port. Prototyped first in Python using the device and protocol documentation, it was then ported C for improved performance. The library has subsequently been open-sourced on GitHub.

The audio-recording functionality was moved to a secondary Raspberry Pi unit to maximise the recorded audio quality which was otherwise impacted by the face-tracking overhead.

Communication between the two Raspberry Pis was done over the network interface, with a purpose-built client/server daemon that managed the activation and deactivation of system elements.

\subsubsection{Learning Objective 3}

The project team have put an emphasis on tracking on going development of project functions. As part of that, system and communication diagrams were created to detail and maintain visibility of the connection points of individual project sections to each other.
 
Some of diagrams and descriptions that can be found throughout their relevant sections of this folio include:

\begin{itemize}

\item \textbf{Kernel and hardware interaction:} Detailing the connectivity and interfaces of external hardware components of the project including PIR sensor, servos, microphone and an external camera.
 
\item \textbf{Processes and interprocess communication:} Having ended up with a solution involving multiple Raspberry Pis, the intercommunication channel between them, as well as the client-server communication functionality which runs over it are detailed and represented in a series of diagrams.
 
\item \textbf{Concurrency and synchronisation:} State of the project system was maintained and synchronised over the communication channel. 
 
\item \textbf{Preemptive and non-preemptive scheduling:} Scheduling became particularly important during investigation into optimal audio recording setup. The original system design utilised only a single Raspberry Pi unit, requiring little interrupts during audio-recording stage. Due to the system overhead of face-detection functionality, a design decision was made to move the audio recording to a separate RPi. For optimal audio recording performance, kernel 3.8.13 recompiled with the real-time kernel patch enabling full preemption. Additionally, the audio recording process was set to -20 nice value.
 
\item \textbf{Memory hierarchy, management, and cost-performance trade-offs:} Significant issues were encountered with getting the face-detection functionality to a perceived real-time state. Substantial evaluation and analysis of potential solutions/configurations and trade offs have been carried out to achieve the prototype state demonstrated in the lab. The topic is further expended on in later sections.
 
\item \textbf{File system design and implementation:} For audio recording, a number optimisations were made to improve the quality of the end-recordings. Some of the optimisation included mounting the FS with noatime flag to turn off Last Access Time recording on file reads, decreasing the "swappiness" value to  introduce a greater delay to storage writes. 

\end{itemize}

More information and in-depth specifics can be found in the relevant sections further in this document.

\subsubsection{Learning Objective 4}

This was a large and complex project with a lot of possible points of failure, as a team we needed to be well organised to ensure the project was completed and working to a production standard. The majority of our success in this assignment came from our approach to treating the project with seriousness and professionalism.

As was outlined in our original project specification (see appendix), our group broke down the project into three main stage which contained weekly deliverables for each member to complete. We endeavoured to order the tasks so that a weekly deliverable was not dependent on the immediate preceding weeks deliverable. This was to mitigate the impact of a task taking longer than expected which would ordinarily delay the overall project deadline. The gantt chart in the appendix illustrates this concept.

Each member had a main responsibility within the team, however our approach to developing the components, which was "prototype, port, integrate, optimise" allowed different group members to experience working on the same component. Similarly to pair programming, it provided a system of multiple code reviews and allowed the experienced members experiment and learn new skills.

Due to time-poor circumstance, we relied heavily on version control, GitHub and regularly discussion to ensure we on target with all our tasks. As our development logs (see appendix) demonstrate we worked project heavily each week and also held meetings to work together on fixing or integrating hardware.




\section{Assumptions and Dependencies}

For this project we were heavily dependent on the open source third party computer vision library - OpenCV.

Due to constraints with the hardware, we've made some general assumptions on how the final product will be used. It is anticipated that the speaker will be presenting in a well lit room, and standing 2-4 metres from device. The speaker is free to move around the room, but their orientation will usually be towards the device.

\section{General Constraints}

The biggest constraint on this project was time. We effective only had 10 weeks to research and develop the prototype, and unfortunately the deadline was non-negotiable. In hindsight we did overestimate what we could achieve by the deadline, but due to our careful planning we were able to still deliver a cutdown but working solution.

After starting the project, another constraint we found quite limiting was using the \rpi\xspace as the foundation device. The CPU processing power of the \rpi\xspace did not allow for non-optimised code to be used, so we had to spend a lot of time refining our solution to perform better. 

Finally, the time constraint coupled with the requirement of using a \rpi, meant we were also constrained also by our collective skill set. We were not familiar with OpenGL ES, but perhaps if we had had more time we could of dedicated time to learning the technology and porting the extensive OpenCV library to work on the \rpi\xspace GPU. 


\section{Development Methodology}

Our approach to developing the project was to divide it into manageable components and assign each part to a individual group member. At the start of each week would communicate what had to be achieved to keep the project on track. 

A lot of care was taken to ensure that the weekly deliverables did not have flow on effects if they were not completed. This was so that time consuming or problematic component did not de-rail other members work if it was not finished on time. We have added a gantt chart to the appendix that demonstrates the lack of dependencies between each week.

\subsection{Programming languages}

For the outset our team decided that we want the system to run efficiently as possible. Therefore it was an obvious choice to use C and C++ as the base language for the software. However, as we were experimenting with technology and techniques we had not used before, we used Python as a way to quickly prototype functionality. Once we were satisfied with the the prototype we would then port the code to C and C++.

Below is the example code we developed for interacting with the PIR-sensor. It was particularly import to prototype because it also involved determining if we had wired it correctly to the \rpi.

\begin{lstlisting}[caption={Python code for interacting with the PIR-sensor},label=pir-sensor-test.py,language=python]

import RPi.GPIO as GPIO
import time
GPIO.setmode(GPIO.BOARD)
GPIO.setup((7), GPIO.IN, pull_up_down=GPIO.PUD_UP)

GPIO.add_event_detect(7, GPIO.RISING)

while True:
    if GPIO.event_detected(7):
        print("Movement")
\end{lstlisting}

Once we knew the Python code was working and the sensor had been wired to the \rpi\xspace correctly, it was trivial to then port it to C++.

\begin{lstlisting}[caption={C++ port of PIR-sensor code},label=pir-sensor-test.cpp,language=C++]
#include <bcm2835.h>

int main(int argc, char **argv)
{
    bcm2835_init();
    bcm2835_gpio_fsel(RPI_GPIO_P1_07, BCM2835_GPIO_FSEL_INPT);
    bcm2835_gpio_set_pud(RPI_GPIO_P1_07, BCM2835_GPIO_PUD_UP);

    while (1) {
        if(bcm2835_gpio_lev(RPI_GPIO_P1_07) != 1) {
            printf("Movement\n");
        }
    }

   bcm2835_close();

   return EXIT_SUCCESS;
}
\end{lstlisting}

When we presented the project to our laboratory class, all production code in the project had been ported to C/C++ except the audio recording daemon. This was due to the time constraints with getting the project ready for demonstration. As a result we saw a time delay executing the daemon, caused by the overhead associated in loading Python to execute the script.


\subsection{Development tools}

Each team member had different preferences of IDE and development tools, but our combined.


\subsubsection{Vim}
    When working off the \rpi, we would occasionally need to tweak some code or write something experimental. Vim was an ideal tool for this, because we be connect over SSH and it would allow us to use regular text editor features (e.g. syntax highlighting). As a long time C++ programmer, Alfred preferred to use Vim on his local machine as well.


\subsubsection{Xcode}

      Compiling on the device was slow, so most of the work was done on a local machine via an IDE. Xcode was ideal because of its built in static analyser and llvm technologies. The static analyser was helpful in diagnosing memory leaks before the code was compiled and running. Writing C/C++ in Xcode meant that we could also take advantage of the advanced clang/clang++ compiler, however on the \rpi\xspace we re-compiled using gcc/g++.

\subsubsection{Instruments}

      Figure \ref{fig:instruments} is a screen shot of how we used Instruments to profile the running applications for performance. We used it to determine what parts of our code were slowest so we could try and optimise those parts.
  
\subsubsection{Git}
      Version control was essential for working on this project. Git was a natural choice because of our shared background with tool.


\begin{figure}
\centering
\includegraphics[width=\textwidth]{graphs/instruments.png}
\caption{Time profiler running to determine longest calling methods.}
\label{fig:instruments}
\end{figure}

\subsection{Collaboration tools}

There were three main collaboration tools we used to progress this project: GitHub, Email and face-to-face meetings.

\subsubsection{GitHub}

From the start we setup a private\footnote{Access to this repository is available upon request.} git repository hosted on GitHub to store all our experimentation, development and production code. Since our project specification and portfolio were typeset in \LaTeX, we were also able to version control that as well. For all other project documentation (such as development logs and bibliography) we took advantage of GitHub's built in wiki to store this information.

GitHub worked for us because git was a version control system we all wanted to use, it was a tool we were familiar with, and it made it easy to identify when a piece of work had been completed.

\subsubsection{Email}

As postgraduate students, we all had competing priorities with our time which meant that we could not work together in person regularly. It was also difficult to meet due to the amount of hardware involved. As a result we often worked in isolation with very defined tasks.

So for us email was key for effective communication and delegating tasks. Once a week a group message was used to talk about our progress and goals for the coming week.

\subsubsection{Face-to-face meetings}

Being all together at once was a rare occasion and we had only a total of five out-of-class-hours group meetings that we all attended. We used the time to visually demonstrate what we had achieved with our tasks and discuss arising issues. 

If another group member was assigned to take over a task (e.g. handing over code to be optimised or integrated), we used this time to instruct in detail how the prototype code worked or how the hardware had been configured. 

\section{Difficulties Encountered}


Throughout the project there were an array of issues we encountered. Below is a summary of some of the significant difficulties we encountered.

\subsubsection{Subject tracking}

Early in the process we discovered that for optimal subject tracking we would need to include "depth sensing" into the project. This was so we could correctly triangulate the position of the subject and accurately reorient the microphone in their direction. However, we worked out that the \rpi\xspace would not be able to handle more than simple image processing. Since this hardware was a constraint of the assignment, our options were limited to face detection, colour detection and background masking. 

The impact of using face detection was that we needed to `guess' how far we would need to shift the microphone when the subject moved. Our solution was to take the centre point of the face and using a grid determine which direction to shift the mount. The servos were painstakingly tuned to shift enough without `overshooting' into the next grid segment. Figure \ref{fig:face} shows an example of our grid solution.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{graphs/face.jpg}
\caption{Left - Face detected outside of centre grid segment. Right - Mount reoriented until face back within centre grid segment.}
\label{fig:face}
\end{figure}

\subsubsection{Hardware issues}

Our project had a lot of extra hardware attached. A PIR-sensor, 2x Servos via a USB servo controller, USB camera, USB sound card and microphone. For the majority of cases the hardware worked perfectly on our local machines and the \rpis. However, the custom C code for interacting with the USB servo controller did not work the same cross-device.

We wrote a custom solution\footnote{Which has been open sourced here: \url{https://github.com/neilang/maestro-wrapper}} for communicating with the \textit{Pololu Maestro Servo Controller} which implemented the command protocol specified on the manufactures website\footnote{\url{http://www.pololu.com/docs/0J40/5.e}}. Unfortunately, unlike OS X, this solution had issues reading data from the device when running on a Linux system. Similar to how we built our other components, we first prototyped this solution in Python, which worked as designed, but when running the same code in a C port the controller would error while reading. The controller did have an in-built facility to diagnose issues through error codes, however to retrieve the error codes we needed to be able to read from the device.

We spent roughly two days debugging this issue, however as time was against us we decided to re-write the solution to avoid reading from the device. We were able to do this because writing to the device still worked fine. As a result we lost the ability to program defensively against this component and just had to assume commands issued to it were returning successfully.

\subsubsection{Audio issues}

\textcolor{green}{TODO: Val could you please help with filling in this part. Stuff about noise on the line etc.}


\subsubsection{Performance tuning}

As mentioned before, we had a lot of trouble ensuring the hardware would be able to handle the CPU requirements of face detection and audio encoding. Our simple solution was to divide the workload over two \rpis, but this brought on the complexity of adding communication to the solution.

Time estimation is a difficult task and in the original project specification we had planned to add more features like speech-to-text translation and a portable power solution. However, tuning the system to run efficiently took up more time than anticipated and we had to re-scope the project to exclude those features. Although we presented a cut-down prototype for the demonstration, we felt it was more important to get the main parts right than deliver slow performing complete solution.


\subsubsection{Team Meetings}

We had a good team with very enthusiastic team members who wanted to do well, however we had a lot of trouble finding time when all members were available to meet. Two of our team worked full-time and the third studied full-time. Meeting was also problematic because of the amount of hardware required to power and connect to our project.

To overcome this we relied heavily on online communication and members delivering their work on time. This method only succeeded because of the dedication of the individuals in the team.


\section{Architecture}

Generally the system architecture followed our original plan outlined in the project specification. However, we did make some minor adjustments to ensure the main application booted on startup and initiated without the need to physically start a recording session.

\subsection{System Design including configuration}

\textcolor{green}{Perhaps some hardware diagrams would work here?}

\subsection{Data Design}

Data storage on the Raspberry Pi is severely limited by the size of the SD card. Although for most applications / use cases for the RPi a typical SD card is sufficient, the long term prospects for usage of our project platform is effected by the audio storage issue.
 
To address this issue, we have investigated various audio formats in search of a balance between audio quality, end filesize, and usability for other project functions such as voice-to-text conversion. Some audio formats considered included WAV (base format), WMA voice, and MP3. Some voice recording specific codecs such as DSP Truespeech although look great in spec, have no open source libraries available to use for this project.
 
For the prototype recording functionality archived audio state, we chose MP3 at 11.5kHz, 16kbps mono as this provided approximately a 5mb/hour filesize which is a huge improvement over WAV which clocked in at approximately 600mb/hour.



\subsection{Program Design}

There were two main running applications for this project, one for each device. The system was setup so that these main programs would initiate when the \rpi\xspace was powered on and no user interaction was required. Figure \ref{fig:processes} illustrates a simplified overview of the main program components.


\begin{figure}
\begin{multicols}{2}
\dirtree{%
.1 server.
.3 pir-monitor.
.3 face-detect.
.3 servo-ctrl.
.3 comm.
}\columnbreak
\dirtree{%
.1 client.
.3 voice-ctrl.
.3 comm.
}
\end{multicols}
\caption{A simplified representation of each programs components.}\label{fig:processes}
\end{figure}

\subsubsection{server and client}

The \textit{server} and \textit{client} were the main running applications. They communicated with each other via a message queue. They responsibilities were for maintaining their sub-components running states.

\subsubsection{pir-monitor}

The \textit{pir-monitor} communicated with the PIR-sensor connected to the devices GPIO ports. It was responsible for keeping a record of last known motion in the room. If no movement was detected for 30 seconds it would signal the server to shutdown recording.

\subsubsection{face-detect}

The \textit{face-detect} component was responsible for communicating with the camera hardware and processing the images for facial images. It would signal the \textit{servo-ctrl} when a new position was detected.

\subsubsection{servo-ctrl}

The \textit{servo-ctrl} component was responsible for communicating with the servo controller hardware. It maintained a record of the servos max, min and current position. It would invoke the move commands on the hardware when a new face position was detected.

\subsubsection{comm}

The communication between components and the devices were handled by a message queue. Messages were sent between devices over UDP.



\subsubsection{Software Design}


The software architecture for this project is basically a Server and Client pattern. The server and client are connecting with Linux sockets using the UDP protocol. The project was divided into many separate sub-components within the server and client, and distributed evenly to balance the CPU performance. Each of the components adopt a message protocol to communicate with each other. To improve the in-dependancy in the code, each component was designed to only do its own job and not be aware of what the other components are doing. Figure \ref{fig:udp} is simple illustration of the protocol.


\begin{figure}
\centering
\includegraphics[width=\textwidth]{graphs/udp.png}
\caption{Communication protocol used by each component.}
\label{fig:udp}
\end{figure}



\subsubsection{Architecture Decisions}

There were a lot of tradeoffs we encountered with designing the software. Due to the size of the project we selected solution that would reduce complexity while still providing reasonable performance.

\begin{description}

  \item[Communication - TCP vs UDP] \hfill \\
      We could of used either TCP or UDP for the inter \rpi\xspace communication. Since we only needed to communicate between two devices which would have a reduced rate of packet loss, we chose UDP. For our project a loss packet would not greatly effect the solution, so this also allowed us to avoid the complexity of managing failed deliveries.
      
  \item[Performance - Idle vs Multi-threaded] \hfill \\
      We took a multi-threaded approach to programming the solution. Many of the components need to wait for communication messages before proceeding, so if we were to idle the functions it would increase the queries between each component and reduce the systems cohesion. With the multi-threaded face detection, the worst case scenario is scanning the whole image and not detecting any faces. As multiple threads are scanning images for faces it can drop system performance dramatically.
  \item[Library - OpenCV vs Pi face detect library] \hfill \\
      There is a RPI Camera board which could be used as an alternative to a USB camera. We obtained a RPI Camera board but choose to use a USB camera because it was easier to debug the face detection code on our local machines and the OpenCV library natively supported USB.

  \item[Thread communication - Pipeline vs Shared memory] \hfill \\
      We choose to use shared memory for inter-thread communication. This was because shared memory provided better communication performance. Due to the number of threads we were using it also reduced the complexity needed to maintain the solution over something like a pipeline.

  \item[Memory - Instance every copy vs Copy on write] \hfill \\
      There is a lot of message data sent within the solution which can be accessed by all components. Instead of each component creating their own copy of a message instance, a copy-on-write method was adopted to avoid wasted object duplication.
      
  \item[Message scheduling - FCFS vs Complex algorithm] \hfill \\
      Every message in the system had the same priority, so first come first serve was the best algorithm to implement and maintain for message scheduling. 
      
\end{description}





\begin{figure}
\centering
\includegraphics[width=\textwidth]{graphs/server-seq.png}
\caption{Sequence diagram demonstrating server component interaction.}
\label{fig:server-seq}
\end{figure}


\begin{figure}
\centering
\includegraphics[width=\textwidth]{graphs/client-seq.png}
\caption{Sequence diagram demonstrating client component interaction.}
\label{fig:client-seq}
\end{figure}




\subsubsection{Sharing memory for threads communication}


Although each thread receives its own stack, they all share the heap memory. Therefore we were able to take advantage of heap to easily communicate between threads using the singleton pattern. We would allocate a pointer in the static memory block that references the heap and give us easy access to the different threads. 

The following code is an example of our singleton implementation.

\begin{lstlisting}[caption={Singleton implementation example},language=C++]
#ifndef __SINGLETON_H__
#define __SINGLETON_H__

template<typename T>
class Singleton {
protected:

  static T *_instance;

public:

  static T* GetInstance()
  {
    if (!_instance) _instance = new T();

    return _instance;
  }
};

template<typename T>
T * Singleton<T>::_instance = 0;

#endif
\end{lstlisting}

\subsubsection{Memory scheduling}

The most difficult part of the message definition is that the length of the message is not know before it is constructed. Due to the memory limits on the \rpi, we were unable to simply create an oversized buffer to cater for all situations. Our solution was to use only create a char array of size one, then use malloc to increase the length dynamically.
The code for dynamically setting the message length is shown below:

\begin{lstlisting}[caption={Example of dynamic message length allocation},language=C++]
struct message_head
{
  int mh_id;
  int mh_size;
  int mh_handle_proxy;
  int mh_reserve;
};

struct message_body
{
  int  mb_len;
  char mb_data[1];
};

struct message
{
  message_head *head;
  message_body *body;
};

void Message::initMessage(int id, string message_data)
{
  _impl->head->mh_id = id;

  _impl->head->mh_handle_proxy = getMessageHandleProxy(id);
  _impl->head->mh_reserve      = 0;

  _impl->body = (message_body *)malloc(sizeof(int) +
                                       sizeof(char) * (message_data.size() + 1));
  _impl->body->mb_len = message_data.size();
    memcpy(_impl->body->mb_data, message_data.c_str(), message_data.size());
  *(_impl->body->mb_data + message_data.size()) = 0;

  _impl->head->mh_size = sizeof(message_head) + sizeof(int)
                         + sizeof(char) * (message_data.size() + 1);
}

\end{lstlisting}




\subsubsection{Copy on write}

The \rpi\xspace has a limited memory size, so it is impossible to realloc memory from the large frame data buffer that come from the face detection frame. Generally, it is a waste of memory to create a local copy of any data when working on resource constrained device.

To make effective use of memory, we adopted a copy-on-write strategy throughout the code base. This strategy only makes a copy of a variable when there modification that needs to be made to the object. This is achieved by always creating and passing pointers to an object rather than the object itself.

Unfortunately, the tradeoff for this approach is that it is not thread-safe. More than one thread may want to access the object and change its original data. To overcome this in our project, we made all the processes (such as face detection and messages) read-only, so a race condition would not occur.



\subsubsection{Message serialisation}

For communication between the server and client we needed to encode the message data. Since the data was of variable length we used `\textbackslash0' as the last character to indicate its length.

As the communication between server and client, we encode the message data into an array that uses `\textbackslash0' at the end. And the message array can also be encoded into message.The code of message serialization is shown as below:
The following code is an example of how data was serialised.

\begin{lstlisting}[caption={Example of message serialisation technique.},language=C++]

Message * Message::createMessage(void *data)
{
  Message *m = new Message();

  int *p = (int *)data;

  m->_impl->head->mh_id           = *(p + 0);
  m->_impl->head->mh_size         = *(p + 1);
  m->_impl->head->mh_handle_proxy = *(p + 2);
  m->_impl->head->mh_reserve      = *(p + 3);
  char *c   = (char *)((p + 4) + 1);
  int   len = strlen(c);
  m->_impl->body         = (message_body *)malloc(
    sizeof(char) * (len + 1) + sizeof(int));
  m->_impl->body->mb_len = *(p + 4);
    memcpy(m->_impl->body->mb_data, c, len);
  m->_impl->body->mb_data[len] = 0;

  return m;
}

void * Message::messageData() const
{
  void *p = malloc(messageLength());

  memset(p, 0, messageLength());
  memcpy(p, _impl->head, sizeof(message_head));
  int *pi = ((int *)p + 4);
  *pi = _impl->body->mb_len;
  char *c = (char *)(pi + 1);
  memcpy(c, _impl->body->mb_data, _impl->body->mb_len);
  c[_impl->body->mb_len] = 0;
  return p;
}
\end{lstlisting}

\subsubsection{Message queue class}

The message queue acts as the communication component in the system. All other components push message to the message queue and it will dispatch messages to appropriate destinations. The most important functionality of the message queue is synchronisation. That is to say, only one thread can push or pop a message at one time in order to prevent race condition. More specifically, the message queue uses a conditional variable to implement protection against race conditions.The core code of the message queue is shown as below.

\begin{lstlisting}[caption={Example of message queue},language=C++]
#include "message_query.h"
#include "message.h"

MessageQuery::MessageQuery()
{
  pthread_mutex_init(&_mutex, NULL);
  pthread_cond_init(&_con, NULL);
}

void MessageQuery::pushMessage(Message *m)
{
  pthread_mutex_lock(&_mutex);
  _query.push_back(m);
  pthread_cond_signal(&_con);
  pthread_mutex_unlock(&_mutex);
}

Message * MessageQuery::popMessage()
{
  pthread_mutex_lock(&_mutex);

  while (_query.empty()) pthread_cond_wait(&_con, &_mutex);

  Message *reVal = _query.front();
  _query.pop_front();
  pthread_mutex_unlock(&_mutex);
  return reVal;
}
\end{lstlisting}

\subsubsection{FCFS strategy}

We took a simple approach to for scheduling messages in the message queue. When one or more messages were pushed into the queue, the dispatching thread would read the first message and send it to the appropriate component. In addition to this, when the queue is empty the dispatching thread will hang up and wait.

The design pattern for the dispatched messages was `chain of responsibility'. The core of the client message dispatching code is shown below.

\begin{lstlisting}[caption={Example of message dispatch},language=C++]
bool Client::dispatchMessage(Message *m)
{
  bool bHandle = false;

  if (m->_impl->head->mh_handle_proxy == proxy_client)
  {
    printf("client dispatching message");

    if (m->_impl->head->mh_id == M_BEGIN_AUDIO_RECORDING) startAudioRecording();

    else if (m->_impl->head->mh_id == M_END_AUDIO_RECORDING) stopAudioRecording();


    bHandle = true;
  }
  else this->sendMessage(m);

  return bHandle;
}
\end{lstlisting}







\subsubsection{Source code or patches for all original work.}


\textcolor{green}{Val to provide patch files for OS configuration.}


\subsubsection{Server Code}

\begin{lstlisting}[caption=basic-service.h,language=C++]
#ifndef __BASE_SERVICE_H__
#define __BASE_SERVICE_H__

#include "message.h"
#include "message_query.h"

class base_service {
public:

  virtual bool setUpService()  = 0;
  virtual void startMainLoop() = 0;
  virtual void stopMainLoop()  = 0;

  virtual void postMessage(Message *m)
  {
    MessageQuery::GetInstance()->pushMessage(m);
  }

  virtual void receiveMessage(const Message& m) = 0;
};

#endif
\end{lstlisting}





\begin{lstlisting}[caption=comm.h,language=C++]
#define LISTEN_THREAD 0
#define WRITE_THREAD  1
#define READ_THREAD   2
#define THREAD_NUM    3

#include <pthread.h>
#include "message_query.h"
#include "message.h"
#include <sys/socket.h>
#include <netinet/in.h>
#include <arpa/inet.h>
#include <stdio.h>
#include <stdlib.h>
#include <errno.h>
#include <string.h>
#include <time.h>
#include <unistd.h>

class Comm {
protected:

  struct sockaddr_in serv_addr;
  int server_socket;
  int client_socket;

  pthread_t threads[THREAD_NUM];
  int  thread_ids[THREAD_NUM];
  bool thread_running[THREAD_NUM];

public:

  MessageQuery *query;

public:

  Comm();
  ~Comm();

  void         createThreads();
  bool         isThreadRunning(int thread_index);

  int          sendMessage(Message *m);
  Message    * readMessage();
  virtual bool dispatchMessage(Message *m) = 0;
};

void         * writeThread(void *arg);
void         * readThread(void *arg);

\end{lstlisting}







\begin{lstlisting}[caption=comm.cpp,language=C++]
#include <sys/socket.h>
#include <netinet/in.h>
#include <arpa/inet.h>
#include <stdio.h>
#include <stdlib.h>
#include <errno.h>
#include <string.h>
#include <time.h>
#include <unistd.h>

#define BUFFER_SIZE 1024

#include <iostream>
using std::cout;        using std::endl;

#include "comm.h"

Comm::Comm()
{
  query = MessageQuery::GetInstance();
  memset(thread_running, 1, THREAD_NUM);
}

Comm::~Comm()
{}

void Comm::createThreads()
{
  thread_ids[WRITE_THREAD] = pthread_create(&threads[WRITE_THREAD],
                                            NULL,
                                            writeThread,
                                            this);

  thread_ids[READ_THREAD] = pthread_create(&threads[READ_THREAD],
                                           NULL,
                                           readThread,
                                           this);
}

bool Comm::isThreadRunning(int thread_index)
{
  return thread_running[thread_index];
}

int Comm::sendMessage(Message *m)
{
  void *p = m->messageData();

  write(client_socket, p, m->messageLength());
  free(p);
  return 0;
}

Message * Comm::readMessage()
{
  char buffer[BUFFER_SIZE] = { 0 };
  int  count               = 0;

  count         = read(client_socket, buffer, sizeof(buffer) - 1);
  buffer[count] = 0;
  Message *m = Message::createMessage(buffer);
  return m;
}

void* writeThread(void *arg)
{
  printf("write thread start\n");

  Comm *s = (Comm *)arg;
  char  buffer[BUFFER_SIZE];

  while (s->isThreadRunning(WRITE_THREAD))
  {
    memset(buffer, 0, BUFFER_SIZE);

    Message *m = MessageQuery::GetInstance()->popMessage();
    s->sendMessage(m);
    delete m;
  }
  return NULL;
}

void* readThread(void *arg)
{
  printf("read thread start\n");

  Comm *s = (Comm *)arg;

  while (s->isThreadRunning(READ_THREAD))
  {
    Message *m = s->readMessage();

    printf("%s\n", m->messageBodyData());

    if (s->dispatchMessage(m)) delete m;
  }
  return NULL;
}
\end{lstlisting}





\begin{lstlisting}[caption=face-detect.h,language=C++]
#include "singleton.h"
#include "base_service.h"
#include <string>
#include <opencv2/opencv.hpp>
using std::string;
using namespace cv;

struct face_detect_arg;
class Message;

class face_detect
  : public Singleton<face_detect>
    , public base_service {
  const char *strWindowName;
  const char *strCascadeClassifier;
  const int   nFrameWaitTime;

  bool isRunning;

  face_detect_arg *fd;

public:

  pthread_t face_main_thread;
  pthread_t face_detect_thread;
  const int nFrameWidth;
  const int nFrameHeight;

public:

  bool             isMainThreadRunning() const;
  const char     * getWindowName() const;
  face_detect_arg* getDetectArg();
  const int        getFrameWaitTime() const;
  const int        getFrameWidth() const;
  const int        getFrameHeight() const;

  pthread_t        getMainThread() const;
  pthread_t        getDetectThread() const;

public:

  face_detect();
  ~face_detect();

  virtual bool setUpService();
  virtual void startMainLoop();
  virtual void stopMainLoop();
  virtual void receiveMessage(const Message& m);

public:

  void       loadCascadeClassifier(const char        *strPath,
                                   CascadeClassifier *pc);
  CvCapture* createCaptureFrameCamera(int nCaptureIndex = -1);
  IplImage * captureFrame(CvCapture *pCapture);
};

struct face_detect_arg
{
  CascadeClassifier *pCvHaar;
  IplImage          *pImage;
  bool               bDetected;
  CvRect             rc;
  bool               threadRunning;
};

void * faceCaptureFunction(void *arg);
void * faceDetectFunction(void *arg);
CvRect detectFaceInImage(IplImage          *inputImg,
                         CascadeClassifier *cascade);

\end{lstlisting}








\begin{lstlisting}[caption=face-detect.cpp,language=C++]
#include <opencv2/opencv.hpp>
#include <string>
#include <iostream>
#include <vector>
#include <assert.h>
#include <pthread.h>
#include "face_detect.h"
#include "message.h"
#include "maestro.h"
#include <stdio.h>
using std::string;      using std::cout;
using std::endl;        using std::vector;
using std::cerr;
using namespace cv;

#define NA_FACE_MOVE_THRESHOLD 400
#define GUI_MODE 0

pthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER;

#define FAILURE_CHECK(p, x)      \
  assert(p);                     \
  if (p == NULL)                 \
  {                              \
    std::cerr << x << std::endl; \
    exit(EXIT_FAILURE);          \
  }

face_detect::face_detect()
  : strWindowName("Face detection test")
    , strCascadeClassifier("lbpcascade_frontalface.xml")
    , nFrameWaitTime(33)
    , nFrameWidth(320)
    , nFrameHeight(240)
    , isRunning(false)
{}

face_detect::~face_detect()
{}

bool face_detect::setUpService()
{
  fd          = (face_detect_arg *)malloc(sizeof(face_detect_arg));
  fd->pCvHaar = new CascadeClassifier();
  loadCascadeClassifier(strCascadeClassifier, fd->pCvHaar);

  fd->pImage        = NULL;
  fd->threadRunning = true;
  fd->bDetected     = false;

  return true;
}

void face_detect::startMainLoop()
{
  isRunning = true;
  pthread_create(&face_main_thread, NULL, faceCaptureFunction, (void *)_instance);
}

void face_detect::stopMainLoop()
{
  isRunning         = false;
  fd->threadRunning = false;
}

void face_detect::receiveMessage(const Message& m)
{}

void face_detect::loadCascadeClassifier(const char        *strPath,
                                        CascadeClassifier *pc)
{
  FAILURE_CHECK(pc, "Conldn't load classifier");

  if (!pc->load(strPath))
  {
    std::cerr << "Conldn't load classifier" << endl;
    exit(EXIT_FAILURE);
  }
}

CvCapture * face_detect::createCaptureFrameCamera(int nCaptureIndex)
{
  CvCapture *p = cvCaptureFromCAM(nCaptureIndex);

  FAILURE_CHECK(p, "Init Capture Failed");

  if (!cvGrabFrame(p))
  {
    std::cerr << "Conldn't GrabFrame" << std::endl;
    exit(EXIT_FAILURE);
  }

  cvSetCaptureProperty(p, CV_CAP_PROP_FRAME_WIDTH,  nFrameWidth);
  cvSetCaptureProperty(p, CV_CAP_PROP_FRAME_HEIGHT, nFrameHeight);
  cvSetCaptureProperty(p, CV_CAP_PROP_FPS,          5);

  return p;
}

IplImage * face_detect::captureFrame(CvCapture *pCapture)
{
  IplImage *p = cvQueryFrame(pCapture);

  FAILURE_CHECK(p, "Error: cvQueryFrame failed");

  return p;
}

CvRect detectFaceInImage(IplImage *inputImg, CascadeClassifier *cascade)
{

  CvSize minFeatureSize = cvSize(80, 80);

  int flags = CV_HAAR_FIND_BIGGEST_OBJECT | CV_HAAR_DO_ROUGH_SEARCH;

  float search_scale_factor = 1.1f;

  IplImage *detectImg = inputImg;

  if (detectImg->nChannels > 1)
  {
    CvSize size       = cvSize(detectImg->width, detectImg->height);
    IplImage *greyImg = cvCreateImage(size, IPL_DEPTH_8U, 1);
    cvCvtColor(detectImg, greyImg, CV_BGR2GRAY);
    detectImg = greyImg;
  }

  vector<Rect> rects;
  cascade->detectMultiScale(detectImg,
                            rects,
                            search_scale_factor,
                            3,
                            flags,
                            minFeatureSize);

  int nFaces = rects.size();

  CvRect rc = cvRect(-1, -1, -1, -1);

  if (nFaces > 0)
  {
    rc = cvRect(rects[0].x, rects[0].y, rects[0].width, rects[0].height);
  }

  if (detectImg->nChannels > 1) cvReleaseImage(&detectImg);

  return rc;
}

double euclideanDistance(CvPoint pt1, CvPoint pt2) {
  double x = pt1.x - pt2.x;
  double y = pt1.y - pt2.y;
  double dist;

  dist = pow(x, 2) + pow(y, 2);
  return dist;
}

void* faceCaptureFunction(void *arg)
{
  face_detect *fdc    = (face_detect *)arg;
  face_detect_arg *fd = fdc->getDetectArg();

  CvCapture *pCapture = fdc->createCaptureFrameCamera();

  IplImage *src =  cvQueryFrame(pCapture);

  pthread_create(&(fdc->face_detect_thread), NULL, faceDetectFunction,
                 (void *)fd);

  float screenDivisions = 2.6;

  Point rightPt2  = Point(fdc->nFrameWidth / screenDivisions,
                          fdc->nFrameHeight - 1);
  Point leftPt1   = Point(fdc->nFrameWidth - (fdc->nFrameWidth / screenDivisions),
                          0);
  Point topPt2    = Point(fdc->nFrameWidth - 1,
                          fdc->nFrameHeight / screenDivisions);
  Point bottomPt1 =
    Point(0, fdc->nFrameHeight - (fdc->nFrameHeight / screenDivisions));

  Maestro *maestro = Maestro::GetInstance();
  maestro->goHome(maestro->horizontalServo);
  maestro->goHome(maestro->verticalServo);
      usleep(1);

  CvPoint lastFace;
  lastFace.x = fdc->nFrameWidth / 2.0;
  lastFace.y = fdc->nFrameHeight / 2.0;

  CvRect rc;
  bool   bDetected = false;

  while (fdc->isMainThreadRunning())
  {
    IplImage *pImg = cvQueryFrame(pCapture);

    if (!pImg)
    {
      std::cerr << "Query Frame Failed" << endl;
      usleep(1000);
      continue;
    }

    assert(pImg);
    pthread_mutex_lock(&mutex);
    fd->pImage = pImg;
    bDetected  = fd->bDetected;
    rc         = fd->rc;
    pthread_mutex_unlock(&mutex);

    CvPoint facePoint;
    facePoint.x = 0;
    facePoint.y = 0;

    if (bDetected)
    {
      CvPoint p1 = cvPoint(rc.x, rc.y);
      CvPoint p2 = cvPoint(rc.x + rc.width, rc.y + rc.height);
      cvRectangle(pImg, p1, p2, CV_RGB(0, 255, 0), 5, 8);

      facePoint.x = rc.x + (rc.width / 2.0f);
      facePoint.y = rc.y + (rc.height / 2.0f);

        printf("Face Detected\n");
    }

    if ((facePoint.x > 0) && (facePoint.y > 0)) {
      double dist = euclideanDistance(facePoint, lastFace);

      if ((dist > 0) && (dist < NA_FACE_MOVE_THRESHOLD))
      {
        printf("send rotate command\n");

        if (facePoint.x < rightPt2.x) maestro->stepUp(maestro->horizontalServo);
        else if (facePoint.x > leftPt1.x) maestro->stepDown(
            maestro->horizontalServo);

        if (facePoint.y < topPt2.y) maestro->stepUp(maestro->verticalServo);

        else if (facePoint.y > bottomPt1.y) maestro->stepDown(
            maestro->verticalServo);
      }

      lastFace.x = facePoint.x;
      lastFace.y = facePoint.y;
    }

    if (cvWaitKey(fdc->getFrameWaitTime()) == 27) fdc->stopMainLoop();

    usleep(1);
  }

  cvReleaseCapture(&pCapture);
  return NULL;
}

void* faceDetectFunction(void *arg)
{
  face_detect_arg *fd = (face_detect_arg *)arg;

  while (fd->threadRunning)
  {
    if (fd->pImage == NULL)
    {
      continue;
    }

    pthread_mutex_lock(&mutex);
    IplImage *pImg = fd->pImage;
    pthread_mutex_unlock(&mutex);

    CvRect rc = detectFaceInImage(pImg, fd->pCvHaar);

    if (rc.x != -1)
    {
      pthread_mutex_lock(&mutex);
      fd->bDetected = true;
      fd->rc        = rc;
      pthread_mutex_unlock(&mutex);
    }
    else
    {
      pthread_mutex_lock(&mutex);
      fd->bDetected = false;
      pthread_mutex_unlock(&mutex);
    }
  }

  return NULL;
}

bool face_detect::isMainThreadRunning() const
{
  return isRunning;
}

const char * face_detect::getWindowName() const
{
  return strWindowName;
}

face_detect_arg * face_detect::getDetectArg()
{
  return fd;
}

pthread_t face_detect::getMainThread() const
{
  return face_main_thread;
}

pthread_t face_detect::getDetectThread() const
{
  return face_detect_thread;
}

const int face_detect::getFrameWaitTime() const
{
  return nFrameWaitTime;
}

const int face_detect::getFrameWidth() const
{
  return nFrameWidth;
}

const int face_detect::getFrameHeight() const
{
  return nFrameHeight;
}

\end{lstlisting}





\begin{lstlisting}[caption=maestro.h,language=C++]
#ifndef __maestro__
#define __maestro__

#include <iostream>
#include <string>
#include <stdio.h>
#include <unistd.h>
#include <fcntl.h>
#include <termios.h>

#include "singleton.h"
#include "base_service.h"

// Source:
// http://stackoverflow.com/questions/134569/c-exception-throwing-stdstring
struct MaestroException : public std::exception
{
  std::string s;
  MaestroException(std::string ss) : s(ss) {}

  ~MaestroException() throw() {}

  const char* what() const throw() {
    return s.c_str();
  }
};

#ifndef __servo__
# define __servo__

class Servo {
private:

  unsigned char  _channel;
  unsigned int   _min;
  unsigned int   _max;
  unsigned short _pos;
  unsigned short _home;

public:

  Servo(unsigned char c, unsigned int mn, unsigned int mx,
        unsigned int hm) : _channel(c), _min(mn), _max(mx), _home(hm) {}

  unsigned char getChannel() {
    return _channel;
  }

  unsigned short getHome() {
    return _home;
  }

  unsigned short getMin() {
    return _min;
  }

  unsigned short getMax() {
    return _max;
  }

  unsigned short getPos() {
    return _pos;
  }

  void setPos(unsigned short p) {
    _pos = p;
  }
};

#endif /* defined(__servo__) */

#ifdef __APPLE__
# define NA_DEVICE "/dev/cu.usbmodem00065291"
#else
# define NA_DEVICE "/dev/ttyACM0"
#endif // ifdef __APPLE__

class Maestro
  : public Singleton<Maestro>
    , public base_service {
private:

  const char *_device;
  int         _fd;

public:

  Servo *horizontalServo;
  Servo *verticalServo;

public:

  Maestro() : _device(NA_DEVICE) {}

  ~Maestro() {
    close(_fd);
  }

  int          getError(Servo *servo);
  int          goHome(Servo *);
  int          getPosition(Servo *);
  int          setPosition(Servo    *,
                           unsigned short);
  int          stepUp(Servo *);
  int          stepDown(Servo *);

  virtual bool setUpService();
  virtual void startMainLoop();
  virtual void stopMainLoop();
  virtual void receiveMessage(const Message& m);

private:

  void openDevice();
};

#endif // ifndef __maestro__

\end{lstlisting}





\begin{lstlisting}[caption=maestro.cpp,language=C++]
#include "maestro.h"
#define NA_STEP_VALUE 150

int Maestro::goHome(Servo *servo) {
  unsigned char command[] = { 0xA2, servo->getChannel() };

  if (write(_fd, command, sizeof(command)) == -1) {
    perror("error writing");
    return -1;
  }

  servo->setPos(servo->getHome());

  return 0;
}

int Maestro::getPosition(Servo *servo) {
  return (int)servo->getPos();
}

int Maestro::setPosition(Servo *servo, unsigned short target) {
  if (target > servo->getMax()) target = servo->getMax();

  if (target < servo->getMin()) target = servo->getMin();
  unsigned char command[] =
  { 0x84, servo->getChannel(), static_cast<unsigned char>(target & 0x7F),
    static_cast<unsigned char>(target >> 7 & 0x7F) };

  if (write(_fd, command, sizeof(command)) == -1) {
    perror("error writing");
    return -1;
  }

  servo->setPos(target);
  return 0;
}

int Maestro::stepUp(Servo *servo) {
  return setPosition(servo, getPosition(servo) + NA_STEP_VALUE);
}

int Maestro::stepDown(Servo *servo) {
  return setPosition(servo, getPosition(servo) - NA_STEP_VALUE);
}

void Maestro::openDevice()
{
  _fd = open(_device, O_RDWR | O_NOCTTY);

  if (_fd == -1) {
    perror(_device);
    throw MaestroException("Invalid Device");
  }
}

bool Maestro::setUpService()
{
  openDevice();
  horizontalServo = new Servo(0, 3968, 9216, 6104);
  verticalServo   = new Servo(1, 3968, 9216, 8220);

  this->goHome(horizontalServo);
  this->goHome(verticalServo);
}

void Maestro::startMainLoop()
{}

void Maestro::stopMainLoop()
{}

void Maestro::receiveMessage(const Message& m)
{}

#undef NA_STEP_VALUE
\end{lstlisting}





\begin{lstlisting}[caption=main-server.cpp,language=C++]
#include "server.h"

int main(int argc, char **argv)
{
  Server *s = Server::GetInstance();

  s->setUpService();
  s->startMainLoop();

  return 0;
}
\end{lstlisting}





\begin{lstlisting}[caption=message.h,language=C++]
#ifndef __MESSAGE_H__
#define __MESSAGE_H__

#include <string>
using std::string;

#define TEST 0

enum message_id
{
  M_START = 0,
  M_END,
  M_DETECTED,
  M_UNDETECTED,

  M_PIR_DETECTED = 100,
  M_PIR_UN_DETECTED,

  M_BEGIN_AUDIO_RECORDING = 200,
  M_END_AUDIO_RECORDING,
};

enum message_handle_proxy
{
  proxy_server = 0,
  proxy_client,
};

struct message_head
{
  int mh_id;
  int mh_size;
  int mh_handle_proxy;
  int mh_reserve;
};

struct message_body
{
  int  mb_len;
  char mb_data[1];
};

struct message
{
  message_head *head;
  message_body *body;
};

class Server;

class Message {
  friend class Server;

  message *_impl;

public:

  Message();
  Message(const Message& other);
  Message& operator=(const Message& rhs);
  Message(void *data, int size);
  ~Message();

public:

  void            initMessage(int    id,
                              string message_data);
  int             messageLength() const;
  void          * messageData() const;
  char          * messageBodyData() const;
  int             getMessageHandleProxy(int id);

  static Message* createMessage(void *data);
};

#endif

\end{lstlisting}





\begin{lstlisting}[caption=message.cpp,language=C++]
#include "message.h"
#include <stdlib.h>
#include <stdio.h>
#include <string.h>

Message::Message()
{
  _impl       = new message();
  _impl->head = new message_head();
  _impl->body = (message_body *)malloc(sizeof(message_body));
}

Message::Message(const Message& other)
{
  _impl       = new message();
  _impl->head = new message_head();
  _impl->body = (message_body *)malloc(sizeof(message_body));

    memcpy(_impl, other._impl, sizeof(message));
}

Message& Message::operator=(const Message& rhs)
{
  if (&rhs != this)
  {
    _impl       = new message();
    _impl->head = new message_head();
    _impl->body = (message_body *)malloc(sizeof(message_body));

    memcpy(_impl, rhs._impl, sizeof(message));
  }

  return *this;
}

Message::~Message()
{
  delete _impl->head;
  delete _impl->body;
  delete _impl;
}

Message * Message::createMessage(void *data)
{
  Message *m = new Message();

  int *p = (int *)data;

  m->_impl->head->mh_id           = *(p + 0);
  m->_impl->head->mh_size         = *(p + 1);
  m->_impl->head->mh_handle_proxy = *(p + 2);
  m->_impl->head->mh_reserve      = *(p + 3);
  char *c   = (char *)((p + 4) + 1);
  int   len = strlen(c);
  m->_impl->body         = (message_body *)malloc(
    sizeof(char) * (len + 1) + sizeof(int));
  m->_impl->body->mb_len = *(p + 4);
    memcpy(m->_impl->body->mb_data, c, len);
  m->_impl->body->mb_data[len] = 0;

  return m;
}

void Message::initMessage(int id, string message_data)
{
  _impl->head->mh_id = id;

  _impl->head->mh_handle_proxy = getMessageHandleProxy(id);
  _impl->head->mh_reserve      = 0;

  _impl->body = (message_body *)malloc(sizeof(int) +
                                       sizeof(char) * (message_data.size() + 1));
  _impl->body->mb_len = message_data.size();
    memcpy(_impl->body->mb_data, message_data.c_str(), message_data.size());
  *(_impl->body->mb_data + message_data.size()) = 0;

  _impl->head->mh_size = sizeof(message_head) + sizeof(int)
                         + sizeof(char) * (message_data.size() + 1);
}

int Message::messageLength() const
{
  return _impl->head->mh_size;
}

void * Message::messageData() const
{
  void *p = malloc(messageLength());

  memset(p, 0, messageLength());
  memcpy(p, _impl->head, sizeof(message_head));
  int *pi = ((int *)p + 4);
  *pi = _impl->body->mb_len;
  char *c = (char *)(pi + 1);
  memcpy(c, _impl->body->mb_data, _impl->body->mb_len);
  c[_impl->body->mb_len] = 0;
  return p;
}

char * Message::messageBodyData() const
{
  return _impl->body->mb_data;
}

int Message::getMessageHandleProxy(int id)
{
  int reVal = -1;

  switch (id)
  {
  case M_START:
  case M_END:
  case M_DETECTED:
  case M_UNDETECTED:
  case M_PIR_DETECTED:
  case M_PIR_UN_DETECTED:
    reVal = proxy_server;
    break;

  case M_BEGIN_AUDIO_RECORDING:
  case M_END_AUDIO_RECORDING:
    reVal = proxy_client;
    break;
  }
  return reVal;
}
\end{lstlisting}





\begin{lstlisting}[caption=message-query.cpp,language=C++]
#ifndef __MESSAGE_QUERY_H__
#define __MESSAGE_QUERY_H__

#include <list>
#include <pthread.h>
#include "singleton.h"
using std::list;

class Message;

class MessageQuery : public Singleton<MessageQuery>{
  list<Message *> _query;

  pthread_mutex_t _mutex;
  pthread_cond_t  _con;

public:

  MessageQuery();
  ~MessageQuery();

  void     pushMessage(Message *m);
  Message* popMessage();

};

#endif

\end{lstlisting}





\begin{lstlisting}[caption=message-query.cpp,language=C++]
#include "message_query.h"
#include "message.h"

MessageQuery::MessageQuery()
{
  pthread_mutex_init(&_mutex, NULL);
  pthread_cond_init(&_con, NULL);
}

MessageQuery::~MessageQuery()
{
  pthread_mutex_destroy(&_mutex);
  pthread_cond_destroy(&_con);
}

void MessageQuery::pushMessage(Message *m)
{
  pthread_mutex_lock(&_mutex);
  _query.push_back(m);
  pthread_cond_signal(&_con);
  pthread_mutex_unlock(&_mutex);
}

Message * MessageQuery::popMessage()
{
  pthread_mutex_lock(&_mutex);

  while (_query.empty()) pthread_cond_wait(&_con, &_mutex);

  Message *reVal = _query.front();
  _query.pop_front();
  pthread_mutex_unlock(&_mutex);
  return reVal;
}
\end{lstlisting}






\begin{lstlisting}[caption=pir-monitor.h,language=C++]
#ifndef __PIR_MONITOR_H__
#define __PIR_MONITOR_H__

#include "singleton.h"
#include "base_service.h"
#include <time.h>

class pir_monitor
  : public Singleton<pir_monitor>
    , public base_service {
  bool   isRunning;
  time_t last_sensor;

public:

  pir_monitor();
  ~pir_monitor();

  virtual bool setUpService();
  virtual void startMainLoop();
  virtual void stopMainLoop();
  virtual void receiveMessage(const Message& m);
};

#endif
\end{lstlisting}






\begin{lstlisting}[caption=pir-monitor.cpp,language=C++]
#include "pir_monitor.h"
#include "face_detect.h"
#include <bcm2835.h>
#include <stdio.h>
#include <stdlib.h>
#include <time.h>
#include <string.h>

#define NA_PIN RPI_GPIO_P1_07
#define NA_POLL_DELAY 33
#define NA_PIN_STILL_VALUE 1

pir_monitor::pir_monitor()
{}

pir_monitor::~pir_monitor()
{
  bcm2835_close();
}

bool pir_monitor::setUpService()
{
  bool reVal = true;

  if (!bcm2835_init()) {
    printf("Can't access hardware, did you use sudo?\n");
    exit(EXIT_FAILURE);
    reVal = false;
  }

  bcm2835_gpio_fsel(NA_PIN, BCM2835_GPIO_FSEL_INPT);

  bcm2835_gpio_set_pud(NA_PIN, BCM2835_GPIO_PUD_UP);

  return reVal;
}

void pir_monitor::stopMainLoop()
{
  isRunning = false;
}

void pir_monitor::startMainLoop()
{
  uint8_t value = NA_PIN_STILL_VALUE;

  time(&last_sensor);
  isRunning = true;

  while (isRunning)
  {
    value = bcm2835_gpio_lev(NA_PIN);

    if (value != NA_PIN_STILL_VALUE)
    {
        printf("dectect movement\n");
      face_detect *fd = face_detect::GetInstance();

      if (!fd->isMainThreadRunning())
      {
        printf("begin face detection");
        fd->startMainLoop();
      }

      time(&last_sensor);

      // Message to start Recording
      char buffer[100];
      memset(buffer, 0, 100);
      sprintf(buffer, "detected: %d\n", (int)last_sensor);
      Message *m = new Message();
      m->initMessage(M_BEGIN_AUDIO_RECORDING, buffer);
      this->postMessage(m);
    }
    else
    {
      time_t new_sensor;
      time(&new_sensor);

      if (difftime(new_sensor, last_sensor) > 30)
      {
        face_detect::GetInstance()->stopMainLoop();

        // Message to stop Recording
        char buffer[100];
        memset(buffer, 0, 100);
        sprintf(buffer, "detected: %d\n", (int)new_sensor);
        Message *m = new Message();
        m->initMessage(M_END_AUDIO_RECORDING, buffer);
        this->postMessage(m);
      }
    }

    delay(NA_POLL_DELAY);
  }

}

void pir_monitor::receiveMessage(const Message& m)
{
}

#undef NA_PIN
#undef NA_POLL_DELAY
#undef NA_PIN_STILL_VALUE

\end{lstlisting}






\begin{lstlisting}[caption=server.h,language=C++]
#include "comm.h"
#include "singleton.h"
#include "base_service.h"

class Server : public Comm
               , public Singleton<Server>
               , public base_service {
  bool isRunning;

public:

  Server();
  ~Server();

  virtual bool setUpService();
  virtual void startMainLoop();
  virtual void stopMainLoop();
  virtual void receiveMessage(const Message& m);
  virtual bool dispatchMessage(Message *m);

private:

  void startPirDetect();
  void startServer();
};

\end{lstlisting}






\begin{lstlisting}[caption=server.cpp,language=C++]
#include "server.h"
#include <sys/socket.h>
#include <netinet/in.h>
#include <arpa/inet.h>
#include <stdio.h>
#include <stdlib.h>
#include <errno.h>
#include <string.h>
#include <time.h>
#include <unistd.h>

#include "face_detect.h"
#include "pir_monitor.h"
#include "maestro.h"

Server::Server()
  : isRunning(true)
{}

Server::~Server()
{}

void Server::startPirDetect()
{
  pir_monitor::GetInstance()->startMainLoop();
}

void Server::startServer()
{
  client_socket = accept(server_socket, (struct sockaddr *)NULL, NULL);
}

bool Server::setUpService()
{
  server_socket = socket(AF_INET, SOCK_STREAM, 0);
  memset(&serv_addr, 0, sizeof(struct sockaddr_in));

  serv_addr.sin_family      = AF_INET;
  serv_addr.sin_addr.s_addr = htonl(INADDR_ANY);
  serv_addr.sin_port        = htons(9999);

  bind(server_socket, (struct sockaddr *)&serv_addr, sizeof(serv_addr));
  listen(server_socket, 10);

  startServer();
  createThreads();

  pir_monitor::GetInstance()->setUpService();

  face_detect::GetInstance()->setUpService();

  Maestro::GetInstance()->setUpService();

  return true;
}

void Server::startMainLoop()
{
  while (isRunning)
  {
    startPirDetect();
    usleep(10);
  }
}

void Server::stopMainLoop()
{
  isRunning = false;
}

void Server::receiveMessage(const Message& m)
{
}

bool Server::dispatchMessage(Message *m)
{
  bool bHandle = false;

  if (m->_impl->head->mh_handle_proxy == proxy_server)
  {
    bHandle = true;
  }
  else this->sendMessage(m);

  return bHandle;
}

\end{lstlisting}






\begin{lstlisting}[caption=singleton.h,language=C++]
#ifndef __SINGLETON_H__
#define __SINGLETON_H__

template<typename T>
class Singleton {
protected:

  static T *_instance;

public:

  static T* GetInstance()
  {
    if (!_instance) _instance = new T();

    return _instance;
  }
};

template<typename T>
T * Singleton<T>::_instance = 0;

#endif
\end{lstlisting}






\begin{lstlisting}[caption=.cpp,language=C++]
\end{lstlisting}




\subsubsection{Client Code}

\begin{lstlisting}[caption=basic-service.h,language=C++]
#ifndef __BASE_SERVICE_H__
#define __BASE_SERVICE_H__

#include "message.h"
#include "message_query.h"

class base_service {
public:

  virtual bool setUpService()  = 0;
  virtual void startMainLoop() = 0;
  virtual void stopMainLoop()  = 0;

  virtual void postMessage(Message *m)
  {
    MessageQuery::GetInstance()->pushMessage(m);
  }

  virtual void receiveMessage(const Message& m) = 0;
};

#endif
\end{lstlisting}




\begin{lstlisting}[caption=client.h,language=C++]
#include "comm.h"
#include "singleton.h"
#include "base_service.h"

class Client : public Comm
               , public Singleton<Client>
               , public base_service {
  pthread_t input_thread;
  bool isRecording;

public:

  bool isRunning;

  Client();
  ~Client();

  virtual bool setUpService();
  virtual void startMainLoop();
  virtual void stopMainLoop();
  virtual void receiveMessage(const Message& m);
  virtual bool dispatchMessage(Message *m);

private:

  void connectToServer();
  int  sendCommand(char *);
  void startAudioRecording();
  void stopAudioRecording();
};
\end{lstlisting}





\begin{lstlisting}[caption=client.cpp,language=C++]
#include "client.h"
#include <sys/socket.h>
#include <netinet/in.h>
#include <arpa/inet.h>
#include <stdio.h>
#include <stdlib.h>
#include <errno.h>
#include <string.h>
#include <time.h>
#include <unistd.h>
#include <stdio.h>
#include <stdlib.h>
#include <fcntl.h>
#include <sys/types.h>
#include <sys/stat.h>

#define HOSTNAME "192.168.1.126"

Client::Client()
  : isRecording(false)
    , isRunning(true)
{}

Client::~Client()
{
  printf("shut down recording");
  sendCommand("shutdown\r");
}

void Client::connectToServer()
{
  if (connect(client_socket, (struct sockaddr *)&serv_addr,
              sizeof(serv_addr)) < 0)
  {
    printf("\n Error: Connect Failed\n");
    exit(1);
  }
}

void* inputFunction(void *arg)
{
  Client *client = (Client *)arg;

  char c;

  scanf("%c", &c);

  if (c == 27) client->isRunning = false;

  return NULL;
}

bool Client::setUpService()
{
  memset(&serv_addr, 0, sizeof(struct sockaddr_in));

  if ((client_socket = socket(AF_INET, SOCK_STREAM, 0)) < 0)
  {
    printf("\n Error: Could not create socket \n");
  }

  serv_addr.sin_family = AF_INET;
  serv_addr.sin_port   = htons(9999);

  if (inet_pton(AF_INET, HOSTNAME, &serv_addr.sin_addr) <= 0)
  {
    printf("\n Error: Connect Failed \n");
    exit(1);
  }

  connectToServer();
  createThreads();


  pthread_create(&input_thread, NULL, inputFunction, (void *)this);

  // TODO: Voice Recode
  return true;
}

void Client::startMainLoop()
{
  while (isRunning)
  {}
}

void Client::stopMainLoop()
{
  isRunning = false;
}

void Client::receiveMessage(const Message& m)
{}

bool Client::dispatchMessage(Message *m)
{
  bool bHandle = false;

  if (m->_impl->head->mh_handle_proxy == proxy_client)
  {
    printf("client dispatching message");

    if (m->_impl->head->mh_id == M_BEGIN_AUDIO_RECORDING) startAudioRecording();

    else if (m->_impl->head->mh_id == M_END_AUDIO_RECORDING) stopAudioRecording();


    bHandle = true;
  }
  else this->sendMessage(m);

  return bHandle;
}

#define FIFO_NAME "/tmp/osp.fifo"

int Client::sendCommand(char *cmd) {
  int fd = open(FIFO_NAME, O_WRONLY);

  if (write(fd, cmd, strlen(cmd)) == -1) {
    perror(":(");
    return -1;
  }

  close(fd);

  return 1;
}

void Client::startAudioRecording()
{
  printf("start Audio Recording\n");

  if (!isRecording)
  {
    sendCommand("start\r");
    isRecording = true;
  }
}

void Client::stopAudioRecording()
{
  printf("start Audio Recording\n");

  if (isRecording)
  {
    sendCommand("stop\r");
    isRecording = false;
  }
}
\end{lstlisting}





\begin{lstlisting}[caption=comm.h,language=C++]
#define LISTEN_THREAD 0
#define WRITE_THREAD  1
#define READ_THREAD   2
#define THREAD_NUM    3

#include <pthread.h>
#include "message_query.h"
#include "message.h"
#include <sys/socket.h>
#include <netinet/in.h>
#include <arpa/inet.h>
#include <stdio.h>
#include <stdlib.h>
#include <errno.h>
#include <string.h>
#include <time.h>
#include <unistd.h>

class Comm {
protected:

  struct sockaddr_in serv_addr;
  int server_socket;
  int client_socket;

  pthread_t threads[THREAD_NUM];
  int  thread_ids[THREAD_NUM];
  bool thread_running[THREAD_NUM];

public:

  MessageQuery *query;

public:

  Comm();
  ~Comm();

  void         createThreads();
  bool         isThreadRunning(int thread_index);

  int          sendMessage(Message *m);
  Message    * readMessage();
  virtual bool dispatchMessage(Message *m) = 0;
};

void         * writeThread(void *arg);
void         * readThread(void *arg);

\end{lstlisting}





\begin{lstlisting}[caption=comm.cpp,language=C++]
#include <sys/socket.h>
#include <netinet/in.h>
#include <arpa/inet.h>
#include <stdio.h>
#include <stdlib.h>
#include <errno.h>
#include <string.h>
#include <time.h>
#include <unistd.h>

#define BUFFER_SIZE 1024

#include <iostream>
using std::cout;        using std::endl;

#include "comm.h"

Comm::Comm()
{
  query = MessageQuery::GetInstance();
  memset(thread_running, 1, THREAD_NUM);
}

Comm::~Comm()
{}

void Comm::createThreads()
{
  thread_ids[WRITE_THREAD] = pthread_create(&threads[WRITE_THREAD],
                                            NULL,
                                            writeThread,
                                            this);

  thread_ids[READ_THREAD] = pthread_create(&threads[READ_THREAD],
                                           NULL,
                                           readThread,
                                           this);
}

bool Comm::isThreadRunning(int thread_index)
{
  return thread_running[thread_index];
}

int Comm::sendMessage(Message *m)
{
  void *p = m->messageData();

  write(client_socket, p, m->messageLength());
  free(p);
  return 0;
}

Message * Comm::readMessage()
{
  char buffer[BUFFER_SIZE] = { 0 };
  int  count               = 0;

  count         = read(client_socket, buffer, sizeof(buffer) - 1);
  buffer[count] = 0;
  Message *m = Message::createMessage(buffer);
  return m;
}

void* writeThread(void *arg)
{
  Comm *s = (Comm *)arg;
  char  buffer[BUFFER_SIZE];

  while (s->isThreadRunning(WRITE_THREAD))
  {
    memset(buffer, 0, BUFFER_SIZE);
    Message *m = MessageQuery::GetInstance()->popMessage();
    s->sendMessage(m);
    delete m;
  }
  return NULL;
}

void* readThread(void *arg)
{
    printf("read thread start\n");

  Comm *s = (Comm *)arg;

  while (s->isThreadRunning(READ_THREAD))
  {
    Message *m = s->readMessage();
    printf("%s\n", m->messageBodyData());

    if (s->dispatchMessage(m)) delete(m);
  }
  return NULL;
}

\end{lstlisting}








\begin{lstlisting}[caption=main-client.cpp,language=C++]
#include "client.h"

int main(int argc, char **argv)
{
  Client *c = new Client();

  c->setUpService();
  c->startMainLoop();
  delete c;

  return 0;
}
\end{lstlisting}







\begin{lstlisting}[caption=message.h,language=C++]
#ifndef __MESSAGE_H__
#define __MESSAGE_H__

#include <string>
using std::string;

enum message_id
{
  M_START = 0,
  M_END,
  M_DETECTED,
  M_UNDETECTED,


  M_PIR_DETECTED = 100,
  M_PIR_UN_DETECTED,

  M_BEGIN_AUDIO_RECORDING = 200,
  M_END_AUDIO_RECORDING,
};

enum message_handle_proxy
{
  proxy_server,
  proxy_client,
};

struct message_head
{
  int mh_id;
  int mh_size;
  int mh_handle_proxy;
  int mh_reserve;
};

struct message_body
{
  int  mb_len;
  char mb_data[1];
};

struct message
{
  message_head *head;
  message_body *body;
};

class Client;

class Message {
  friend class Client;
  message *_impl;

public:

  Message();
  Message(const Message& other);
  Message& operator=(const Message& rhs);
  Message(void *data, int size);
  ~Message();

public:

  void            initMessage(int    id,
                              string message_data);
  int             messageLength() const;
  void          * messageData() const;
  char          * messageBodyData() const;
  int             getMessageHandleProxy(int id);

  static Message* createMessage(void *data);
};

#endif
\end{lstlisting}







\begin{lstlisting}[caption=message.cpp,language=C++]
#include "message.h"
#include <stdlib.h>
#include <stdio.h>
#include <string.h>

Message::Message()
{
  _impl       = new message();
  _impl->head = new message_head();
  _impl->body = (message_body *)malloc(sizeof(message_body));
}

Message::Message(const Message& other)
{
  _impl       = new message();
  _impl->head = new message_head();
  _impl->body = (message_body *)malloc(sizeof(message_body));

    memcpy(_impl, other._impl, sizeof(message));
}

Message& Message::operator=(const Message& rhs)
{
  if (&rhs != this)
  {
    _impl       = new message();
    _impl->head = new message_head();
    _impl->body = (message_body *)malloc(sizeof(message_body));

    memcpy(_impl, rhs._impl, sizeof(message));
  }

  return *this;
}

Message::~Message()
{
  delete _impl->head;
  delete _impl->body;
  delete _impl;
}

Message * Message::createMessage(void *data)
{
  Message *m = new Message();

  int *p = (int *)data;

  m->_impl->head->mh_id           = *(p + 0);
  m->_impl->head->mh_size         = *(p + 1);
  m->_impl->head->mh_handle_proxy = *(p + 2);
  m->_impl->head->mh_reserve      = *(p + 3);
  char *c   = (char *)((p + 4) + 1);
  int   len = strlen(c);
  m->_impl->body         = (message_body *)malloc(
    sizeof(char) * (len + 1) + sizeof(int));
  m->_impl->body->mb_len = *(p + 4);
    memcpy(m->_impl->body->mb_data, c, len);
  m->_impl->body->mb_data[len] = 0;

  return m;
}

void Message::initMessage(int id, string message_data)
{
  _impl->head->mh_id = id;

  _impl->head->mh_handle_proxy = getMessageHandleProxy(id);
  _impl->head->mh_reserve      = 0;

  _impl->body = (message_body *)malloc(sizeof(int) +
                                       sizeof(char) * (message_data.size() + 1));
  _impl->body->mb_len = message_data.size();
    memcpy(_impl->body->mb_data, message_data.c_str(), message_data.size());
  *(_impl->body->mb_data + message_data.size()) = 0;

  _impl->head->mh_size = sizeof(message_head) + sizeof(int)
                         + sizeof(char) * (message_data.size() + 1);
}

int Message::messageLength() const
{
  return _impl->head->mh_size;
}

void * Message::messageData() const
{
  void *p = malloc(messageLength());

  memset(p, 0, messageLength());
  memcpy(p, _impl->head, sizeof(message_head));
  int *pi = ((int *)p + 4);
  *pi = _impl->body->mb_len;
  char *c = (char *)(pi + 1);
  memcpy(c, _impl->body->mb_data, _impl->body->mb_len);
  c[_impl->body->mb_len] = 0;
  return p;
}

char * Message::messageBodyData() const
{
  return _impl->body->mb_data;
}

int Message::getMessageHandleProxy(int id)
{
  int reVal = -1;

  switch (id)
  {
  case M_START:
  case M_END:
  case M_DETECTED:
  case M_UNDETECTED:
  case M_PIR_DETECTED:
  case M_PIR_UN_DETECTED:
    reVal = proxy_server;
    break;

  case M_BEGIN_AUDIO_RECORDING:
  case M_END_AUDIO_RECORDING:
    reVal = proxy_client;
    break;
  }
  return reVal;
}
\end{lstlisting}







\begin{lstlisting}[caption={message\_query.h},language=C++]
#ifndef __MESSAGE_QUERY_H__
#define __MESSAGE_QUERY_H__

#include <list>
#include <pthread.h>
#include "singleton.h"
using std::list;

class Message;

class MessageQuery : public Singleton<MessageQuery>{
  list<Message *> _query;

  pthread_mutex_t _mutex;
  pthread_cond_t  _con;

public:

  MessageQuery();
  ~MessageQuery();

  void     pushMessage(Message *m);
  Message* popMessage();
};

#endif
\end{lstlisting}







\begin{lstlisting}[caption=message-query.cpp,language=C++]
#include "message_query.h"
#include "message.h"

MessageQuery::MessageQuery()
{
  pthread_mutex_init(&_mutex, NULL);
  pthread_cond_init(&_con, NULL);
}

MessageQuery::~MessageQuery()
{
  pthread_mutex_destroy(&_mutex);
  pthread_cond_destroy(&_con);
}

void MessageQuery::pushMessage(Message *m)
{
  pthread_mutex_lock(&_mutex);
  _query.push_back(m);
  pthread_cond_signal(&_con);
  pthread_mutex_unlock(&_mutex);
}

Message * MessageQuery::popMessage()
{
  pthread_mutex_lock(&_mutex);

  while (_query.empty()) pthread_cond_wait(&_con, &_mutex);

  Message *reVal = _query.front();
  _query.pop_front();
  pthread_mutex_unlock(&_mutex);
  return reVal;
}
\end{lstlisting}







\begin{lstlisting}[caption=singleton.h,language=C++]
#ifndef __SINGLETON_H__
#define __SINGLETON_H__

template<typename T>
class Singleton {
protected:

  static T *_instance;

public:

  static T* GetInstance()
  {
    if (!_instance) _instance = new T();

    return _instance;
  }
};

template<typename T>
T * Singleton<T>::_instance = 0;

#endif

\end{lstlisting}






\begin{lstlisting}[caption=osp-project-client.service,language=bash]
[Unit]
Description=OSP Project Daemon
After=syslog.target 

[Service]
Type=oneshot
User=audio
Group=audio
ExecStart=/usr/local/bin/osp_client
ExecStart=/usr/bin/python /var/osp/pipe.py
ExecStop=/bin/killall osp_client
ExecStop=/bin/killall python
Restart=on-abort

[Install]
WantedBy=multi-user.target
\end{lstlisting}


\begin{lstlisting}[caption=limits.conf,language=bash]
*               -       rtprio          0
*               -       nice            0
@audio          -       rtprio          65
@audio          -       nice           -25
@audio          -       memlock         40000
\end{lstlisting}


\begin{lstlisting}[caption=audio-daemon.py,language=python]
#!/usr/bin/env python

import sys
import os
import time
import getopt
import atexit
import alsaaudio
from daemon import Daemon

def get_fname():
    prefix = "/root/audio/output/recording_"
    return prefix + time.strftime("%d-%m-%y_%H-%M", time.gmtime()) + '.wav'

def shutdown(daemon, pipename):
    daemon.cleanup()
    os.remove(pipename)
    print("Shutting down")
    
class Recording(Daemon):
    def cleanup(self):
        # convert to mp3 and die
        pass

    def run(self):
        card = 'default'
        f = open(get_fname(), 'wb')

        inp = alsaaudio.PCM(alsaaudio.PCM_CAPTURE, alsaaudio.PCM_NONBLOCK, card)

        # Set attributes: Mono, 44100 Hz, 16 bit little endian samples
        inp.setchannels(1)
        inp.setrate(44100)
        inp.setformat(alsaaudio.PCM_FORMAT_S16_LE)

        inp.setperiodsize(160)
        while True:
           # Read data from device
            l, data = inp.read()
          
            if l:
                f.write(data)
                time.sleep(.001)
        f.close()

if __name__ == "__main__":
    named_pipe = '/tmp/osp.fifo'
    command = ''
    daemon = Recording('/tmp/recording_daemon.pid')

    try:
        os.mkfifo(named_pipe)
    except OSError:
        print("Error creating pipe, guess it exists")
        #sys.exit(1)

    while command != 'shutdown':
        pipe = open(named_pipe, 'r')
        command = pipe.read()[:-1]
        print("command received: %s" % command)
        if command == 'start':
            print("starting recording")
            a = os.fork()
            if a == 0:
                daemon.start()
        if command == 'stop':
            print("stopping recording")
            daemon.stop()
        if command == 'shutdown':
            print("stopping recording")
            os.kill(a, 0)
            daemon.stop()
            pipe.close()

    atexit.register(shutdown, daemon, named_pipe)
\end{lstlisting}



\section{Testing Issues}

Testing our system was both hard and slow. The real world contextual behaviour of our project, our program did not lend itself to the traditional unit testing pattern.

Our testing was mostly based around behaviour and performance. This was largely a manual process. For example, we would alter the stepping value for the servos, compile the change on the device, then review what impact that had on the quality of movement in the product. It was repetitive and painstakingly slow task. We performed similar testing for face detection, audio recording quality, motion detection, and servo movement.



\subsection{Testing Done}


\begin{table}
\begin{tabular}{|c|c|c|}
    \hline
    \textbf{Trained Set} & \textbf{FPS} & \textbf{Accuracy} \\ \hline
    
    Frontal face & 13 & Acceptable \\ \hline
    
    Eye & 13-14 & Erratic \\ \hline
    
    Mouth & 13-14 & False positives \\ \hline
    
    Nose & 11 & Low detection, false positives \\ \hline
    
    Ear & 14 & No detection \\ \hline
    
    Eye pair & 14 & No detection \\ \hline

\end{tabular}
\caption{Trained data sets}
\label{table:parts}
\end{table}

In most of our testing we would review performance and accuracy. Table \ref{table:parts} lists the results from testing different trained facial data sets. Although we were able to increase the frames per second with different facial features, the accuracy of detection decreased or was erratic and unusable.

Besides each owning a \rpi, we only had one physical copy of each hardware part we needed. This meant that while one member was working on a piece of hardware, the others could not test their code against it.

To overcome this limitation we came up with the concept of artificially simulating the hardware using software. The below code is an example of a program we wrote to simulate the camera and face detection component detecting faces and passing it to the servo controllers.

\begin{lstlisting}[caption=rand\_face\_detect.cpp,language=C++]
#include <iostream>
#include <cstdlib>
#include <unistd.h>
#include <stdio.h>
#include <string.h>

#define NA_DEFAULT_WAIT 3
#define NA_MIN_TOLERANCE 2
#define NA_DEFAULT_TOLERANCE 5

#define MAX(X,Y) ((X) > (Y) ? (X) : (Y))

int rand_num()
{
    return (rand() % 200 + 1) - 100;
}

int rand_with_tolerance(int t, int l)
{
    setvbuf(stdout, NULL, _IONBF, 0);
    int i;
    do {
        i = rand_num();
    } while (i > l+t || i < l-t);
    return i;
}

int main(int argc, const char * argv[])
{
    int x = 0;
    int y = 0;

    int t = NA_DEFAULT_TOLERANCE;
    int s = NA_DEFAULT_WAIT;

    for (int i = 1; i < argc; i++) {
        int v = i+1 <= argc;
        if (strcmp(argv[i], "-t") == 0 && v) {
            t = std::stoi(argv[i+1]);
            t = MAX(t, NA_MIN_TOLERANCE);
        }
        else if (strcmp(argv[i], "-s") == 0 && v) {
            s = std::stoi(argv[i+1]);
            s = MAX(s, 0);
        }
        else if (strcmp(argv[i], "-h") == 0) {
            std::cout << "Usage: -t <tolerance> -s <seconds>" << std::endl;
            return EXIT_SUCCESS;
        }

    }

    while (true) {
        x = rand_with_tolerance(t, x);
        y = rand_with_tolerance(t, y);
        printf("%d,%d\n", x, y);
        sleep(s);
    }

    return EXIT_SUCCESS;
}
\end{lstlisting}


\subsection{Performance Experiments}

Similar to testing, performance evaluation required us to run the entire the program to measure the effect of changes. Figure \ref{fig:chart} illustrates a performance experiment we did with resizing an image before processing it. The reduced resolutions increased the frames per second, up until 160x120 where it had no impact.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{graphs/chart.png}
\caption{FPS at different image sizes.}
\label{fig:chart}
\end{figure}

The hardest part about experiment with our system to improve performance was that we had to ensure it still responded as design. For example, some code changes would improve the CPU usage but decrease subject detection rate. These things were hard to measure and record.

\section{Roles and Responsibilities}

As outlined in our original project specification, there was overlap in our teams individual strengths so the roles were divided with some shared responsibilities. Although at different levels, we wanted all three group members to be involved with experimenting with the hardware, writing software and documenting. The cross-over in roles made the progress slightly slower, but allowed each of us to trial something new.

\subsection{Val Lyashov}

Val was our hardware and unix expert. He was primarily responsible for the physical design of the solution, but he was also involved in prototyping some of the software-hardware interfaces.

\subsection{"Alfred" Yang Yuan}

Alfred was our lead software developer. He has extensive experience in C++ development and was able to architect all the components together and optimise their performance.

\subsection{Neil Ang}

Neil was designated leader of the group, but was also involved in porting the software components to C/C++, presenting and documenting the project. He researched and prototyped the early face detection code.


\section{Breakdown of Work Done by Team Member}

Each team member kept a detailed log of their activities. Below is a summary of what they achieved each week.

\subsection{Val Lyashov}

See appendix for daily log of tasks.

\begin{description}

  \item[Week 2] \hfill \\
    Formed team with Neil and discussed ideas for the project. Brain stormed a few areas we have common interest in. Began researching existing projects on similar veins.
  \item[Week 3] \hfill \\
    Narrowed down the project focus and problem we would be tackling.
    Third member we ended up with in Week 2, showed little interest in aiming outside of immediate comfort zone, and we (Neil and myself) felt wasn't a good fit. This was communicated and we parted ways.
    Began acquiring hardware components for prototyping/experimentation.
    Started to break down the project into tasks and commenced development.
  \item[Week 4] \hfill \\
   Acquired new team member, Alfred.
   Hardware began to arrive, and hardware interface development kicked off.
  \item[Week 5] \hfill \\
    Demonstrated cross-compile milestone in lab.
    Basic servo functionality working, writing interface and developing camera mount. Started running into issues with power supply to drive servos, using Arduino's 5V rail for temporary power.

  \item[Week 6] \hfill \\
    Microphone and external soundcards have arrived. Began working on recording functionality. Huge audio quality issues encountered.
    Issues with getting smooth servo transitions via software control over GPIO pins. Ordered external servo controller for experimentation.
    Getting PIR sensor working on the RPi.
  \item[Week 7] \hfill \\
    External servo controller arrived. No suitable device driver found, prototyped one in Python based on the propriatry protocol documentation. Shows a lot more promise vs. original software control. Testing external battery pack to drive the servos via the external controller.
    Have recompiled the Linux kernel on the RPi for dedicated for audio recording, including the real-time kernel patch for full preemption. Notable audio quality improvements.

  \item[Week 8] \hfill \\
    Took a trip to Bunnings with Neil to build the servo mount. Base assembly and servo harness put together. Not pretty but works. Constructed the microphone mounting point. Weight is an issue on the servos even though the specs say they are ratefd for 5kg load.
  \item[Week 9] \hfill \\
    Putting all compontents together, helping out with the integration of communications between the 2 RPis, stabilising the power supply for the servos.
     Working on imrpoving the audio recording functionality and experimenting with voice-to-text via Google's Speech API (part of Chrome). Doesn't appear to like Australian accents.
  \item[Week 10] \hfill \\
    Configuring on-boot initiation of all system compontents, including connectivity logic. Written services for systemd, and daemonising the running components.
    Began running through the presentation format.
 
  \item[Week 11] \hfill \\
    Working on documentation and project folio.
    Making some of the developed code more generic and open sourcing, namely the Pololu controller bit.
  \item[Week 12] \hfill \\
     Working on project folio.

\end{description}


\subsection{"Alfred" Yang Yuan}

\begin{description}

  \item[Week 3] \hfill \\
      Bought a \rpi. Tried to find a great team aiming for a HD in this course.

  \item[Week 4] \hfill \\
      Joined the team. Read the document and the some source of OpenCV. Then bought an USB camera to run Neil's program on \rpi.
  \item[Week 5] \hfill \\
      Demonstrated cross-compile milestone in lab. Tested two different algorithm to improve performance of the face detection. The two algorithms did not increase the performance dramatically. So I put the face detection algorithm on another thread, in order to make the program run faster.
  \item[Week 6] \hfill \\
      Investigated the implementation of PIR motion sensing. Started to code first version for communication protocol between two \rpis. 
  \item[Week 7] \hfill \\
       Implemented serialisation and synchronisation in communication and copy-on-write in message serialisation. Implemented message scheduling, including FIFS and message dispatch.
  \item[Week 8] \hfill \\
       Wrote the software architecture of this project. Tried to connect all the parts in the project with well defined messaging.
  \item[Week 9] \hfill \\
       Wrote the Client part of the project, and connected the Server and Client via 
		  existing message dispatch process. Increased the face detection performance to make it run much more faster on \rpi. Combined Val's recording code with a pipe.
  \item[Week 10] \hfill \\
      Presented project to lab group (see appendix). Started work on final project portfolio submission.
  \item[Week 11] \hfill \\
      Worked on final project portfolio submission. Wrote partial document about what the architecture, and explained why I built the software like this. In addition, added some comments for some programming tricks.
  \item[Week 12] \hfill \\
      Continued work on project portfolio submission. Cleaned the code for the final submission.

\end{description}

\subsection{Neil Ang}

See appendix for daily log of tasks.

\begin{description}

  \item[Week 2] \hfill \\
      Formed team with Val and discussed ideas for the project. Purchased \rpi. Completed cross-compile milestone. Installed Arch linux on \rpi. Started research into computer vision libraries.
  \item[Week 3] \hfill \\
      Purchased a RPi camera board. Installed Raspbian on \rpi. Researched depth sensing on the device. Prototyped face detection code on MBP with OpenCV. Acquired a web camera and tested on the device. Voted for a team restructure.
  \item[Week 4] \hfill \\
      Acquired new team member. Continued experimenting with OpenCV to improve performance running on the device. Started work on project specification.
  \item[Week 5] \hfill \\
      Demonstrated cross-compile milestone in lab. Continued work on project specification. Wrote test program to simulate face detection for Val.
  \item[Week 6] \hfill \\
      Finished the project specification. Ported the PIR motion sensing Python code to C.
  \item[Week 7] \hfill \\
      Researched IPC techniques. Acquired the servos and started work on porting code to C++.
  \item[Week 8] \hfill \\
      Solved issues with servo powers. Finished work on C++ wrapper for servos. Got face detection based movement working. Took a trip to Bunnings with Val to build the servo mount.
  \item[Week 9] \hfill \\
      Debugged issues with the servo wrapper when running on the Pi. Wrote simple daemon for automatically starting the server/client code. Wrote presentation and speech for next milestone.
  \item[Week 10] \hfill \\
      Presented project to lab group (see appendix). Started work on final project portfolio submission.
  \item[Week 11] \hfill \\
      Worked on final project portfolio submission.
  \item[Week 12] \hfill \\
      Continued work on project portfolio submission.

\end{description}


\section{Summary and Conclusions}

Throughout the project we endeavoured to meet all the learning objectives. Our selection of development tools, complex system design, extensive documentation and teamwork were designed with this in mind. It was an ambitious project that tested the abilities of each individual in the group. As a team we worked tirelessly to deliver a solution that was both functioning and maintained a acceptable level of quality.

There are still hurdles to overcome, but we believe the project could have commercial potential.

\textcolor{red}{This feels unfinished... don't know what else to write... maybe add  a picture?}


\begin{appendices}


\chapter{Project Specification}
\includepdf[pages=-]{appendix/specification.pdf}


\chapter{Gantt Chart}
\includepdf[fitpaper=true,pages=-]{appendix/gantt-chart.pdf}


\chapter{Prototype Demonstration - Speech}

\newpage

Hi, we're team $\langle$sql injection$\rangle$. We are Alfred, Val and Neil.Because the raspberry pi is small and portable, we wanted to build something that would take advantage of this. We also liked the idea of building something that would respond to its surroundings. So we decided on an ambitious project to modernise the phonograph (commonly known as a dictaphone), but combining subject tracking with targeted audio recording equipment.

Here you can see Alfred, demoing the project. As he moves the left/right/up/down the Pi detects his new position and reorients the the shotgun microphone. This particular microphone is designed for targeted audio, and will record sound up to 3 metres while excluding background noise. The intended use of this product would be for recording lectures and tutorials, and producing podcasts. Our original design also included audio-to-text conversion, so we could generate transcripts, but the translation library we were using didn’t work well with Australian or Russian or Chinese accents... so... we dropped that part.

The first learning objective was about design, development and debugging a complex program on the Pi. With three group members with different backgrounds, we had different preferences for building the project. Our approach was to modularise the solution and allow multiple members work on the same thing but in different languages. For example, the servo code was quickly prototyped in python to ensure it functioned correctly. Once it was working, another group member re-wrote the python into C++ code. As most of you would have come up against, compiling on the Pi was slow. So a lot of early development was done on laptops then later tested on the device. There were a few circumstances where code would work on our machines and not the device, but that was due to the differing versions of g++ we were using and not a big issue to fix. A bonus of working this way, was that we had access to IDE debuggers and static analysers. While trying to improve the performance of our code, we ran the program through "Instruments", which detected where the code was the slowest and also picked up on a memory leak. Since our project was about interacting with the physical world (i.e detecting faces, moving motors, sensing motion). We had to run a lot of manual benchmarking. Basically we would tweak the degree of movement in servos, re-run the code and roughly evaluate if it was getting better or worse.Finally, all source code was checked into git, and hosted on GitHub. We heart GitHub. We used it for our source code, project specification and to record our development logs and bibliography.

Leaning objective 2 was about assessing trade-offs in hardware on a constrained system. To make things easier we put decided to split the workload over two Pis. One for face detection and movement, the other for audio recording and processing. For the subject tracking we looked at multiple solutions. We first thought about using depth sensing with something like a MS Kinect sensor, but found out that it was too resource intensive, so we were limited to 2D visual processing, and settled on face detection because of the suitability for the solution. We could also have done colour tracking or background masking.To perform real-time face detection you take a snapshot from the camera, convert it to grayscale, equalise the histogram, then use a trained classifier (in our case Haar-like features) to scan the image for face shapes at different scales, repeat.There is a C++ based open source library called OpenCV, which implements the classifier we wanted to use. It also supports GPU processing, but only NVIDIA GPUs (which the Pi doesn’t have). So to run it on the Pi we had to do the whole thing on CPU. This gave us the challenge of finding ways to optimise the CPU-based face detection so that it would run in real-time and actually detect faces. We took an iterative approach to this.So we started a basic face detection script and ran it. It used 100\% of the CPU and took about a 15 seconds to process each frame. We were using a 720p camera, so the first obvious step was to reduce the frame size so we were processing to less. At 320x240 we able to process a frame every 4-5 seconds and dropped CPU usage down to 70\%. We also added some limits on how it searched, by giving it a minimum face size, and set it to only perform a rough search for the biggest face. We got it at around 50\% CPU and 2-3 seconds per frame. Not bad.Alfred, then had some brilliant ideas on improve the performance further. Original script was creating a matrix of pixels that were copied into RAM and processed, he switched the code to use the pointers from the cameras instead of rebuilding the pixel matrix locally, and processed them on background threads. When the camera moves the frames that are still being processed are dropped, because the position has changed. Also, because of the way we predict the program would be used, we could reduce the pixels to process even further, by remembering the last face position and targeting just that area first. With all the additions we got the face detection and movement happening at an acceptable speed. There are some limitation to the implementation, such as moving too fast for the camera, or recording a profile shot - but we chalk this up to the constraints of using a raspberry pi. We ran the same scripts on a modern day MBP, and the accuracy and speed were at least 4 times better.A face is a complex shape, so we also thought about detecting just an eye or a nose. We ran some  tests, which improved the overall FPS, but the accuracy was a lot less. So full face detection was our best option. 

Also in attempting to improve the overall performance of project modules (eg face detection, real time recording). We investigated modification to process scheduling, however due to the bare bones nature of our system, there was no competition for CPU time.

Learning objective 4 was about team work. This project was particularly hard to work on as a team. Like other teams in this class would have come across, we faced with the obstacle of limited time for such an ambitious project. Two of our members work full-time, and the other studies full-time, so we had to do a lot of work at night in isolation. The obvious solution was to modularise the project into components, and have each member deliver each week. As can be seen in the table, we also tried to reduce dependencies on deliverables, so if one ran over time it wouldn’t impact the next weeks deliverables.But our biggest team issue with the amount of hardware we were using. Besides the Pis, we only had purchased one microphone, one motion sensor, one set of servos etc. So only one team member had physical access to a piece of hardware at a time. To overcome this obstacle, we came up with the idea of writing hardware simulation scripts. For example, we wrote a C++ program that would simulate face detection and output fake coordinates - This meant Val could fine tune the motor movement without having the physical camera or the face detection code complete.Another problem with the amount of hardware, was that when we did meet together, we needed to bring a lot of equipment. The photo on the slide doesn’t really do justice to what it was like to haul 3x laptops, 3x pis, a router, networking cables, servo cables, PIR sensor breadboards, plus tools etc.Finally, as a team we also wanted to do cross-skill development, which made things slower but increased our personal learning. Each members primary role was designed to make use of their best strengths. But we also included cross-overs in the task so we could each try something new. For example, although Val was our expert hardware guy, Neil (who had very limited hardware experience) got to write some of the C interfaces to the hardware, which is something he had never done before. We did similar things with all roles so that we each learned something new.

\chapter{Prototype Demonstration - Handout}
\includepdf[pages=-]{appendix/summary.pdf}



\chapter{Prototype Demonstration - Slides}
\includepdf[fitpaper=true,pages=-]{appendix/presentation.pdf}


\chapter{Neil's Development Log}
\include{appendix/devlog-neil}

\chapter{Val's Development Log}
\include{appendix/devlog-val}


\chapter{Peer review feedback}
\newpage
\addtocounter {page} {46}

\chapter{Milestone 1}
\newpage
\addtocounter {page} {2}


\end{appendices}

\nocite{*}
\printbibliography[heading=bibintoc]


\end{document}