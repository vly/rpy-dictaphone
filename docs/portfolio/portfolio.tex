\documentclass[11pt,a4paper,titlepage]{report}


% Document settings

\title{OSP Portfolio \\ Team $\langle$sql injection$\rangle$}

\author{
  Neil Ang\\
  \texttt{s3251533}
  \and
  ``Alfred" Yang Yuan\\
  \texttt{s3363619}
  \and
  Val Lyashov\\
  \texttt{s3366222}
}

\date{Semester 2, 2013}

% Set watermark while in development
\usepackage{draftwatermark}
\SetWatermarkText{DRAFT}
\SetWatermarkScale{5}
\SetWatermarkColor[gray]{0.95}


% Change section numbering
%\renewcommand\thesection{\Roman{section}}
%\renewcommand\thesubsection{\Alph{subsection}}
\renewcommand\thesection{\arabic{section}}
\renewcommand\thesubsection{\thesection.\arabic{subsection}}


% Enable smart quotes
\usepackage [english]{babel}
\usepackage [autostyle]{csquotes}
\MakeOuterQuote{"}

% Alias pi name
\usepackage{xspace}
\newcommand{\rpi}{\textit{Raspberry Pi\textsuperscript{\textregistered}}}
\newcommand{\rpis}{\textit{Raspberry Pi\textsuperscript{\textregistered}s}}

% Side by side graphics
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

% Switch to biblatex
\usepackage{biblatex}
\bibliography{computer-vision}
\bibliography{audio}
\bibliography{servo}

% Add the bib to the toc
\DefineBibliographyStrings{english}{
  bibliography = {Bibliography},
}

% Better table height
\usepackage{tabu}

% The appendix
\usepackage{appendix}
\usepackage{pdfpages}


% Code highlighting
\usepackage{listings}

\lstset{
    language=C++,
    basicstyle=\small\sffamily,
    numbers=left,
    numberstyle=\tiny,
    frame=tb,
    columns=fullflexible,
    showstringspaces=false
}


% For links
\usepackage{hyperref}



% For marking what's left to do
\usepackage{color}


\begin{document}


\maketitle

\pagebreak
\tableofcontents
\thispagestyle{empty}
\pagebreak

\section{Introduction}

Live presentations audio recording is a well known problem for organisers, speakers and attendees alike. While there are significant amount of solutions available, factors such as cost, complexity and portability often limit widespread adoption. While existing services such as Echo360 and Lectopia work very well in recoding in a lecture theatre environment, ad-hoc deployment in smaller venues can be costly and logistically difficult to achieve.
 
In this project we present a cost-effective, light-weight presentation recording solution that requires little setup time. Designed with flexibility and rapid ad-hoc deployment as the guiding imperatives, the solution presents a good alternative for both personal and organisational use.


\section{Goals and Objectives}


The focus of the project has been to deliver an easy-to-use, fast to setup, and cost-effective solution to live presentation recording. The following summary provides an overview of the core goals and projective we have strived towards over the course of the product development. The priority figure is the value on a scale of 1 to 5 (low to high respectively) that the team have placed on the goal/objective (particularly during the initial prototype phase).


\begin{description}

  \item[Cost-effectiveness (Priority 5)] \hfill \\
      One of the main factors that determines the viability of the end-product is the overall cost effectiveness. The aim is to provide a sub-\$200 product in a self-assembly state.
  \item[Ease of use (Priority 4)] \hfill \\
      The product has to be easy to use and operate by end-users. This includes setup, operation, troubleshooting stages of use. Some technical knowledge would be a required for the current form of the product.
  \item[Semi-portability (Priority 3)] \hfill \\
      As the project aims to primarily cater for ad-hoc recording scenarios, portability plays an important part of the overall system design. This takes form in complexity of assembly, size and weight of the finished product, the power usage and requirement, as well as the storage capacity of the recording functionality.
  \item[Off-the-shelf hardware (Priority 2)] \hfill \\
      To make the final product accessible to the largest share of prospective audience, the availability of components that make up the product becomes critical for the commercialisation of the project.
  \item[Modularity (Priority 3)] \hfill \\
      Partially related to the off-the-shelf hardware objective, modularity of components and accessories would ensure greater usage and types of operating environments the product can be deployed in.

\end{description}

\textcolor{red}{TODO: Add the learning objectives to this list. They were mentioned in the project spec as a goal.}


\section{Feedback and Self-Reflection}

We were fortunate to have formed a very focused team with complimentary skill sets. The feedback we received was very much a reflection of our hard work and determination. Overall we are very proud of what we have achieved.


\subsection{Self Assessment}

From the start we knew this would be an ambitious project. We wanted to build something impressive, useful and which would overly meet the learning objectives. On reflection we did over-estimated what we could achieve in the time frame, however the positive feedback from what we did accomplish have made us still regard this project as a success.

Each team member brought a vital skill set to the project. Val's hardware and unix knowledge gave us the building blocks for creating the project, Neil's programming skills created working prototypes for each component, and Alfred's optimisation ability polished the project and brought all the pieces together. Although we needed these skills to develop the solution, it was the hard working nature of all of the team members that was the real backbone of the project.

\textcolor{red}{TODO: add sentence about each team member challenging their own ability and expanding their knowledge.}


During the lab demonstration we were asked the question "What mark do you think you deserve?" Our response was "HD" and we still stand by this answer. We treated all aspects of the project seriously and put in the hours needed to achieve the end solution. We delivered each milestone on time and didn't compromise on the quality of our work.


At the end of this project there are a number of design changes we would make during the production of a second prototype. These include:

\begin{itemize}
    \item \textbf{Stronger servos:} The servos used for the project, although worked OK supporting the weight of the usb camera, struggled with the extra weight introduced by the shotgun microphone, cables, component mount and clamps. Spending approximately an additional 20% more money would provide better servos which come standard with metal gear kits.

    \item \textbf{Mounts and clamps:} The team have gone through various versions of camera and microphone mount prototypes before ending up with the final one used during the presentation. Although solid and fully adjustable, weight was a major tradeoff. The central shaft also reduced the vertical movement by approximately 30 degrees. A dedicated pan/swing mounting assembly as those used for dslr/movie camera mounts would provide greater range and smoothness of movements.

    \item \textbf{Camera:} Although the USB camera used for the project ended up working quite well, the resource constraints of the Raspberry Pi have forced the team to squeeze efficiency for face detection to work in real-time. These included significantly reducing the frame capture size and number of features detected. A camera with greater hardware-level control over the output (e.g. monochrome capture mode) would help to further reduce the resource overhead. The native Raspberry Pi camera module seams like a good candidate, however, the current drivers and documentation are not mature enough for achieving the required results in this instance.

    \item \textbf{Power source:} Having gone through a number of issues of getting a consistent external power source to drive the servos via the USB servo controller, and trying to manage the cable spaghetti caused by all the components a single power source to drive all the parts of this project would go a long way to resolve these issues. External laptop battery packs, although fairly heavy, do provide consistent power to run the project for at least 6 hours. The added bonus, is the size of such a battery pack could also serve as a base for the servo assembly.
\end{itemize}


\subsection{Summary of Prototype Demonstration}

Due to the hard work of all team members we were able to demonstrate a working prototype in our Week 10 laboratory. The presented product included an auto-startup feature so no laptop or router were needed for setup. We were able to successfully demonstrate to the class our face tracking, audio recording and motion sensing components of the solution. 

Although what we presented was a cutdown version of what was proposed in the project specification (missing the speech-to-text translation), we were very proud of what we had achieved and the positive reception the project received from our peers.

In preparation for the  demonstration we developed a short slide show, written speech and summary handout. All of these are available in the appendix. We also created a video of the working product incase we encountered technical issues during live demo. 

\begin{center}
\href{http://www.youtube.com/watch?v=hckEBpT1VcU}{
    \includegraphics[width=\textwidth]{graphs/youtube.png}
}
\end{center}
See the video here: \url{http://www.youtube.com/watch?v=hckEBpT1VcU}



\subsection{Peer Review Summary}

The reviews from our classmates during the presentation were very positive. There have been encouragement from a number of students and our lab assistant to continue development of this project towards a commercially viable state. 

Some suggestions offered by the peer feedback forms as well as verbally included:

\begin{itemize}
    \item demonstrate the inter workings of the RPis (a behind-the-scenes view of sorts) through a streaming debug activity to an external monitor
    \item debug audio monitoring capability
    \item move to a better spec ARM board such as the Beagleboard to improve the performance of face detection
\end{itemize}


\subsection{Self Reflection / Lessons Learned}

Although we had a talented group, each individual team member took something away from this project.

\subsubsection{Val}

Having never worked with a Raspberry Pi before, I found the project quite enlightening, particularly in understanding the lower-level hardware interfacing through GPIO pins. 

The project also provided an awesome opportunity for me to learn off the other team members concepts and techniques I would otherwise encounter only as theory. It's been great to have such a wide knowledge base to draw upon when working towards troubleshooting and resolving hardware and software issues.

The work involving the servos and the Pololu servo controller was also the first time I had a chance to prototype a hardware device library from API documentation, which has subsequently greatly increased my confidence in own abilities.

Lastly, being required to work on a resource constrained device (Raspberry Pi) has really highlighted the value of measurement and optimisation of system performance, which provided real-world context a lot of the topics covered during our lectures.

\subsubsection{Alfred}

During the lab, I learnt how to use linux command, and how to use cross-complier to build a linux image.

I used a lot multi-threading programming, as a result of that I am now quiet familiar with Synchronisation in a linux way. In addition, the face detection had requirement on memory management, so I spent a long time to investigate how to implement this in C++. Also, the message dispatch helped me understand about how to schedule jobs.

Finally, thanks for Neil and Val, they help me a lot in this project. This is definitely the most successful group work for me while at RMIT.


\subsubsection{Neil}

This project gave me an excellent opportunity to try a lot of new things. Working on the \rpi\xspace and building a subject tracking system was something that I had never done before.


I had also never written a interface to device hardware before. It was very hard to debug but very satisfying once it was working. This also gave me the opportunity to expand my C and C++ experience.

The face detection was also very fun to work with. It was surprising easy to program and it has given me ideas for other potential projects where I could use this.


As a team I think we functioned very well. I normally try to avoid teamwork but our collection of skills and determination meant we delivered a project of a high standard.


Finally, this projec also gave me a chance to create two new open source projects. While researching how to interact with our hardware I found very little code examples available. In an effort to help others, I decided to open source hardware based components of the project.


\subsection{Description of how each learning objective is addressed}


Over the course of the project development, this team have made significant leaps in their knowledge of computer-hardware interfacing, operating system level functionality and application optimisation in a resource constrained environment. This section highlights how various learning objectives of the OSP course have been met.
 
\subsubsection{Learning Objective 1}
For the duration of this project, all of our ongoing development code has been managed through Github to ensure team members were always using the latest stable code when working on their assigned tasks.

With the face detection (computer vision) generating major CPU load when running on the Raspberry Pi, the team has gone through a significant amount of iterations of the functionality in order to achieve real-time response to the enviroment.


For efficiency during the initial development stages, most hardware interface interaction (e.g. servo controller, PIR sensor etc.) were prototyped using the Python programming language, and later ported into C when performance optimisation was required.


Debugging and performance has been used extensively over the course of the project for both hardware and software components. For Python, Pudb was the debugger of choice. For C/C++ Instruments and LLDB/GDB were utilised. On the hardware side, servo controller built-in error-checking was utilised for all communication to ensure hardware errors were caught and dealt with.

\textcolor{red}{TODO: Open sourced project to show mastery: https://github.com/neilang/pir-monitor}

\textcolor{red}{TODO: Open sourced project to show mastery: https://github.com/neilang/maestro-wrapper}


 
\subsubsection{Learning Objective 2}

Working with the Raspberry Pi, our team encountered significant challenges in producing a functional project prototype while having so little processing power available at our disposal.

Significant amount of trial and error was carried out in optimising performance of face tracking functionality of the our project (as can be observed by the Github repository history). 

The target(s) coordinates derived from face-tracking functionality were then used to tilt and pan the assembly via two servos at the base of the camera and microphone combo. Initial experimentation with control of servos via Raspberry Pi's GPIO pins shown to be too inaccurate and resulting servo movement too jagged to be used due to CPU interrupts effecting the clock cycle count. The chosen solution was to acquire an external hardware servo controller, the Pololu Maestro. Running into issues with available libraries; our team put together a control library to communicate with the controller over the virtual  serial port. Prototyped first in Python using the device and protocol documentation, it was then ported C for improved performance. The library has subsequently been open-sourced on GitHub.

The audio-recording functionality was moved to a secondary Raspberry Pi unit to maximise the recorded audio quality which was otherwise impacted by the face-tracking overhead.

Communication between the two Raspberry Pis was done over the network interface, with a purpose-built client/server daemon that managed the activation and deactivation of system elements.

\subsubsection{Learning Objective 3}

\textcolor{green}{Val, are you working on this learning objective?}

\subsubsection{Learning Objective 4}

\textcolor{red}{Neil, you are working on this learning objective!}



\section{Assumptions and Dependencies}

\textcolor{red}{TODO: This is a confusing heading... perhaps we can review what we had in the project specification here?}


\section{General Constraints}

\textcolor{red}{TODO: Time, skills, hardware.}


\section{Development Methodology}

\textcolor{red}{TODO: Discuss our background using agile and how we differ from SCRUM. Reinforce how we met learning objective 1 here.}


\subsection{Programming languages}

For the outset our team decided that we want the system to run efficiently as possible. Therefore it was an obvious choice to use C and C++ as the base language for the software. However, as we were experimenting with technology and techniques we had not used before, we used Python as a way to quickly prototype functionality. Once we were satisfied with the the prototype we would then port the code to C and C++.

Below is the example code we developed for interacting with the PIR-sensor. It was particularly import to prototype because it also involved determining if we had wired it correctly to the \rpi.

\begin{lstlisting}[caption={Python code for interacting with the PIR-sensor},label=pir-sensor-test.py,language=python]

import RPi.GPIO as GPIO
import time
GPIO.setmode(GPIO.BOARD)
GPIO.setup((7), GPIO.IN, pull_up_down=GPIO.PUD_UP)

GPIO.add_event_detect(7, GPIO.RISING)

while True:
    if GPIO.event_detected(7):
        print("Movement")
\end{lstlisting}

Once we knew the Python code was working and the sensor had been wired to the \rpi\xspace correctly, it was trivial to then port it to C++.

\begin{lstlisting}[caption={C++ port of PIR-sensor code},label=pir-sensor-test.cpp,language=C++]
#include <bcm2835.h>

int main(int argc, char **argv)
{
    bcm2835_init();
    bcm2835_gpio_fsel(RPI_GPIO_P1_07, BCM2835_GPIO_FSEL_INPT);
    bcm2835_gpio_set_pud(RPI_GPIO_P1_07, BCM2835_GPIO_PUD_UP);

    while (1) {
        if(bcm2835_gpio_lev(RPI_GPIO_P1_07) != 1) {
            printf("Movement\n");
        }
    }

   bcm2835_close();

   return EXIT_SUCCESS;
}
\end{lstlisting}

When we presented the project to our laboratory class, all production code in the project had been ported to C/C++ except the audio recording daemon. This was due to the time constraints with getting the project ready for demonstration. As a result we saw a time delay executing the daemon, caused by the overhead associated in loading Python to execute the script.


\subsection{Development tools}

Each team member had different preferences of IDE and development tools, but our combined.


\subsubsection{Vim}
    When working off the \rpi, we would occasionally need to tweak some code or write something experimental. Vim was an ideal tool for this, because we be connect over SSH and it would allow us to use regular text editor features (e.g. syntax highlighting). As a long time C++ programmer, Alfred preferred to use Vim on his local machine as well.


\subsubsection{Xcode}

      Compiling on the device was slow, so most of the work was done on a local machine via an IDE. Xcode was ideal because of its built in static analyser and llvm technologies. The static analyser was helpful in diagnosing memory leaks before the code was compiled and running. Writing C/C++ in Xcode meant that we could also take advantage of the advanced clang/clang++ compiler, however on the \rpi\xspace we re-compiled using gcc/g++.

\subsubsection{Instruments}

      Figure \ref{fig:instruments} is a screen shot of how we used Instruments to profile the running applications for performance. We used it to determine what parts of our code were slowest so we could try and optimise those parts.
  
\subsubsection{Git}
      Version control was essential for working on this project. Git was a natural choice because of our shared background with tool.


\begin{figure}
\centering
\includegraphics[width=\textwidth]{graphs/instruments.png}
\caption{Time profiler running to determine longest calling methods.}
\label{fig:instruments}
\end{figure}

\subsection{Collaboration tools}

There were three main collaboration tools we used to progress this project: GitHub, Email and face-to-face meetings.

\subsubsection{GitHub}

From the start we setup a private\footnote{Access to this repository is available upon request.} git repository hosted on GitHub to store all our experimentation, development and production code. Since our project specification and portfolio were typeset in \LaTeX, we were also able to version control that as well. For all other project documentation (such as development logs and bibliography) we took advantage of GitHub's built in wiki to store this information.

GitHub worked for us because git was a version control system we all wanted to use, it was a tool we were familiar with, and it made it easy to identify when a piece of work had been completed.

\subsubsection{Email}

As postgraduate students, we all had competing priorities with our time which meant that we could not work together in person regularly. It was also difficult to meet due to the amount of hardware involved. As a result we often worked in isolation with very defined tasks.

So for us email was key for effective communication and delegating tasks. Once a week a group message was used to talk about our progress and goals for the coming week.

\subsubsection{Face-to-face meetings}

Being all together at once was a rare occasion and we had only a total of five out-of-class-hours group meetings that we all attended. We used the time to visually demonstrate what we had achieved with our tasks and discuss arising issues. 

If another group member was assigned to take over a task (e.g. handing over code to be optimised or integrated), we used this time to instruct in detail how the prototype code worked or how the hardware had been configured. 

\section{Difficulties Encountered}


Throughout the project there were an array of issues we encountered. Below is a summary of some of the significant difficulties we encountered.

\subsubsection{Subject tracking}

Early in the process we discovered that for optimal subject tracking we would need to include "depth sensing" into the project. This was so we could correctly triangulate the position of the subject and accurately reorient the microphone in their direction. However, we worked out that the \rpi\xspace would not be able to handle more than simple image processing. Since this hardware was a constraint of the assignment, our options were limited to face detection, colour detection and background masking. 

The impact of using face detection was that we needed to `guess' how far we would need to shift the microphone when the subject moved. Our solution was to take the centre point of the face and using a grid determine which direction to shift the mount. The servos were painstakingly tuned to shift enough without `overshooting' into the next grid segment. Figure \ref{fig:face} shows an example of our grid solution.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{graphs/face.jpg}
\caption{Left - Face detected outside of centre grid segment. Right - Mount reoriented until face back within centre grid segment.}
\label{fig:face}
\end{figure}

\subsubsection{Hardware issues}

Our project had a lot of extra hardware attached. A PIR-sensor, 2x Servos via a USB servo controller, USB camera, USB sound card and microphone. For the majority of cases the hardware worked perfectly on our local machines and the \rpis. However, the custom C code for interacting with the USB servo controller did not work the same cross-device.

We wrote a custom solution\footnote{Which has been open sourced here: https://github.com/neilang/maestro-wrapper} for communicating with the \textit{Pololu Maestro Servo Controller} which implemented the command protocol specified on the manufactures website\footnote{http://www.pololu.com/docs/0J40/5.e}. Unfortunately, unlike OS X, this solution had issues reading data from the device when running on a Linux system. Similar to how we built our other components, we first prototyped this solution in Python, which worked as designed, but when running the same code in a C port the controller would error while reading. The controller did have an in-built facility to diagnose issues through error codes, however to retrieve the error codes we needed to be able to read from the device.

We spent roughly two days debugging this issue, however as time was against us we decided to re-write the solution to avoid reading from the device. We were able to do this because writing to the device still worked fine. As a result we lost the ability to program defensively against this component and just had to assume commands issued to it were returning successfully.

\subsubsection{Audio issues}

\textcolor{red}{TODO: Talk to Val about filling in this part}


\subsubsection{Performance tuning}

As mentioned before, we had a lot of trouble ensuring the hardware would be able to handle the CPU requirements of face detection and audio encoding. Our simple solution was to divide the workload over two \rpis, but this brought on the complexity of adding communication to the solution.

Time estimation is a difficult task and in the original project specification we had planned to add more features like speech-to-text translation and a portable power solution. However, tuning the system to run efficiently took up more time than anticipated and we had to re-scope the project to exclude those features. Although we presented a cut-down prototype for the demonstration, we felt it was more important to get the main parts right than deliver slow performing complete solution.


\subsubsection{Team Meetings}

We had a good team with very enthusiastic team members who wanted to do well, however we had a lot of trouble finding time when all members were available to meet. Two of our team worked full-time and the third studied full-time. Meeting was also problematic because of the amount of hardware required to power and connect to our project.

To overcome this we relied heavily on online communication and members delivering their work on time. This method only succeeded because of the dedication of the individuals in the team.


\section{Architecture}

\textcolor{red}{TODO: add intro summary about architecture}

\subsection{System Design including configuration}

\textcolor{green}{Perhaps some hardware diagrams would work here?}

\subsection{Data Design}

\textcolor{red}{TODO: Drop this heading? I think we did in the project spec.} \textcolor{green}{Val, do you want to add anything about audio formats here?}

\subsection{Program Design}


The software architecture for this project is basically a Server and Client pattern. The server and client are connecting with Linux sockets using the UDP protocol. The project was divided into many separate sub-components within the server and client, and distributed evenly to balance the CPU performance. Each of the components adopt a message protocol to communicate with each other. To improve the in-dependancy in the code, each component was designed to only do its own job and not be aware of what the other components are doing. Figure \ref{fig:udp} is simple illustration of the protocol.


\begin{figure}
\centering
\includegraphics[width=\textwidth]{graphs/udp.png}
\caption{Communication protocol used by each component.}
\label{fig:udp}
\end{figure}



\subsubsection{Architecture Decisions}

There were a lot of tradeoffs we encountered with designing the software. Due to the size of the project we selected solution that would reduce complexity while still providing reasonable performance.

\begin{description}

  \item[Communication - TCP vs UDP] \hfill \\
      We could of used either TCP or UDP for the inter \rpi\xspace communication. Since we only needed to communicate between two devices which would have a reduced rate of packet loss, we chose UDP. For our project a loss packet would not greatly effect the solution, so this also allowed us to avoid the complexity of managing failed deliveries.
      
  \item[Performance - Idle vs Multi-threaded] \hfill \\
      We took a multi-threaded approach to programming the solution. Many of the components need to wait for communication messages before proceeding, so if we were to idle the functions it would increase the queries between each component and reduce the systems cohesion. With the multi-threaded face detection, the worst case scenario is scanning the whole image and not detecting any faces. As multiple threads are scanning images for faces it can drop system performance dramatically.
  \item[Library - OpenCV vs Pi face detect library] \hfill \\
      There is a RPI Camera board which could be used as an alternative to a USB camera. We obtained a RPI Camera board but choose to use a USB camera because it was easier to debug the face detection code on our local machines and the OpenCV library natively supported USB.

  \item[Thread communication - Pipeline vs Shared memory] \hfill \\
      We choose to use shared memory for inter-thread communication. This was because shared memory provided better communication performance. Due to the number of threads we were using it also reduced the complexity needed to maintain the solution over something like a pipeline.

  \item[Memory - Instance every copy vs Copy on write] \hfill \\
      There is a lot of message data sent within the solution which can be accessed by all components. Instead of each component creating their own copy of a message instance, a copy-on-write method was adopted to avoid wasted object duplication.
      
  \item[Message scheduling - FCFS vs Complex algorithm] \hfill \\
      Every message in the system had the same priority, so first come first serve was the best algorithm to implement and maintain for message scheduling. 
      
\end{description}





\begin{figure}
\centering
\includegraphics[width=\textwidth]{graphs/server-seq.png}
\caption{Sequence diagram demonstrating server component interaction.}
\label{fig:server-seq}
\end{figure}


\begin{figure}
\centering
\includegraphics[width=\textwidth]{graphs/client-seq.png}
\caption{Sequence diagram demonstrating client component interaction.}
\label{fig:client-seq}
\end{figure}




\subsubsection{Sharing memory for threads communication}


Although each thread receives its own stack, they all share the heap memory. Therefore we were able to take advantage of heap to easily communicate between threads using the singleton pattern. We would allocate a pointer in the static memory block that references the heap and give us easy access to the different threads. 

The following code is an example of our singleton implementation.

\begin{lstlisting}[caption={Singleton implementation example},language=C++]
#ifndef __SINGLETON_H__
#define __SINGLETON_H__

template<typename T>
class Singleton {
protected:

  static T *_instance;

public:

  static T* GetInstance()
  {
    if (!_instance) _instance = new T();

    return _instance;
  }
};

template<typename T>
T * Singleton<T>::_instance = 0;

#endif
\end{lstlisting}

\subsubsection{Memory scheduling}

The most difficult part of the message definition is that the length of the message is not know before it is constructed. Due to the memory limits on the \rpi, we were unable to simply create an oversized buffer to cater for all situations. Our solution was to use only create a char array of size one, then use malloc to increase the length dynamically.
The code for dynamically setting the message length is shown below:

\begin{lstlisting}[caption={Example of dynamic message length allocation},language=C++]
struct message_head
{
  int mh_id;
  int mh_size;
  int mh_handle_proxy;
  int mh_reserve;
};

struct message_body
{
  int  mb_len;
  char mb_data[1];
};

struct message
{
  message_head *head;
  message_body *body;
};

void Message::initMessage(int id, string message_data)
{
  _impl->head->mh_id = id;

  _impl->head->mh_handle_proxy = getMessageHandleProxy(id);
  _impl->head->mh_reserve      = 0;

  _impl->body = (message_body *)malloc(sizeof(int) +
                                       sizeof(char) * (message_data.size() + 1));
  _impl->body->mb_len = message_data.size();
    memcpy(_impl->body->mb_data, message_data.c_str(), message_data.size());
  *(_impl->body->mb_data + message_data.size()) = 0;

  _impl->head->mh_size = sizeof(message_head) + sizeof(int)
                         + sizeof(char) * (message_data.size() + 1);
}

\end{lstlisting}




\subsubsection{Copy on write}

PI has little memory size, so it is impossible to reallco memory for passing large frame data buffer that come from face detection frame. More specify, it turn to be waste of memory coping more than one instance of any kind of instance in the project. In order to reduce the usage of memory, we adopt the Copy on write strategy. This strategy only makes a copy when there is some modification on the object, and the most of time they shall only create a new pointer point to the original address.The trade-off for this strategy is that it is no thread-safe, because more than one thread want to access the object when about to change the original data. However, in our project, all the processes, such as face detection and messages, are read-only process, which means that we do not have to consider race condition when we use this approach. The code of message is shown as below:


\textcolor{red}{TODO: INSERT CODE}

\subsubsection{Message serialization}

As the communication between server and client, we encode the message data into an array that uses '\textbackslash0' at the end. And the message array can also be encoded into message.The code of message serialization is shown as below:

\textcolor{red}{TODO: INSERT CODE}


\subsubsection{Message queue class}

The message queue acts as the communication component in the system. All other components push message to the message queue and the dispatch messages to appropriate component. The most important functionality for message queue should be synchronization. That is to say, only one thread can push or pop message at the same time in order to prevent race condition. More specific, the message queue use conditional valuable to control the race condition. The core code of message queue is shown as below:

\textcolor{red}{TODO: INSERT CODE}


\subsubsection{FCFS strategy}

The code shows that we use the simplest strategy to scheduling the message, which means that when there is a message push into the message queue, the dispatching thread shall read the first message in the message queue send to the appropriate component to serve it. In addition, the dispatch thread shall be hinge up and wait when there are no messages in message queue.The message shall be dispatched messages using design pattern: chain of responsibility. The process base on the message ID, which have been introduced in message protocol part. The dispatch component shall query message id and push the message to the serve components.The core code of client message dispatching is shown as below:

\textcolor{red}{TODO: INSERT CODE}





\subsubsection{Software Design}

\textcolor{red}{TODO: rehash the structure we used in the project spec}


\subsubsection{Source code or patches for all original work.}


\textcolor{green}{Val to provide patch files for OS configuration.}

\textcolor{red}{TODO: Add description to each source file.}


\subsubsection{Server Code}

\include{server-source}

\subsubsection{Client Code}

\include{client-source}


\section{Testing Issues}

\textcolor{red}{TODO: slow, hard.}


\subsection{Testing Done}

\textcolor{red}{TODO: discuss hardware sharing issues, slowness, and bench marking of things}

\begin{lstlisting}[caption=rand-face-detect.cpp,language=C++]
#include <iostream>
#include <cstdlib>
#include <unistd.h>
#include <stdio.h>
#include <string.h>

#define NA_DEFAULT_WAIT 3
#define NA_MIN_TOLERANCE 2
#define NA_DEFAULT_TOLERANCE 5

#define MAX(X,Y) ((X) > (Y) ? (X) : (Y))

int rand_num()
{
    return (rand() % 200 + 1) - 100;
}

int rand_with_tolerance(int t, int l)
{
    setvbuf(stdout, NULL, _IONBF, 0);
    int i;
    do {
        i = rand_num();
    } while (i > l+t || i < l-t);
    return i;
}

int main(int argc, const char * argv[])
{
    int x = 0;
    int y = 0;

    int t = NA_DEFAULT_TOLERANCE;
    int s = NA_DEFAULT_WAIT;

    for (int i = 1; i < argc; i++) {
        int v = i+1 <= argc;
        if (strcmp(argv[i], "-t") == 0 && v) {
            t = std::stoi(argv[i+1]);
            t = MAX(t, NA_MIN_TOLERANCE);
        }
        else if (strcmp(argv[i], "-s") == 0 && v) {
            s = std::stoi(argv[i+1]);
            s = MAX(s, 0);
        }
        else if (strcmp(argv[i], "-h") == 0) {
            std::cout << "Usage: -t <tolerance> -s <seconds>" << std::endl;
            return EXIT_SUCCESS;
        }

    }

    while (true) {
        x = rand_with_tolerance(t, x);
        y = rand_with_tolerance(t, y);
        printf("%d,%d\n", x, y);
        sleep(s);
    }

    return EXIT_SUCCESS;
}
\end{lstlisting}


\subsection{Performance Bounds}

\textcolor{red}{TODO: Not sure about this heading... perhaps rehash whats in the project spec}

\subsection{Performance Experiments}



\textcolor{red}{TODO: Make table a reference figure.}


\begin{center}
\begin{table}
\begin{tabular}{|c|c|c|}
    \hline
    \textbf{Trained Set} & \textbf{FPS} & \textbf{Accuracy} \\ \hline
    
    Frontal face & 13 & Acceptable \\ \hline
    
    Eye & 13-14 & Erratic \\ \hline
    
    Mouth & 13-14 & False positives \\ \hline
    
    Nose & 11 & Low detection, false positives \\ \hline
    
    Ear & 14 & No detection \\ \hline
    
    Eye pair & 14 & No detection \\ \hline

\end{tabular}
\end{table}
\end{center}




\section{Roles and Responsibilities}

As outlined in our original project specification, there was overlap in our teams individual strengths so the roles were divided with some shared responsibilities. Although at different levels, we wanted all three group members to be involved with experimenting with the hardware, writing software and documenting. The cross-over in roles made the progress slightly slower, but allowed each of us to trial something new.

\subsection{Val Lyashov}

\textcolor{red}{Add paragraph about hardware solution}

\subsection{"Alfred" Yang Yuan}

\textcolor{red}{Add paragraph about software solution}

\subsection{Neil Ang}

\textcolor{red}{Add paragraph about whatever is left}


\section{Breakdown of Work Done by Team Member}

Each team member kept a detailed log of their activities. Below is a summary of what they achieved each week.

\subsection{Val Lyashov}

See appendix for daily log of tasks.

\begin{description}

  \item[Week 2] \hfill \\
      Formed team with Neil and discussed ideas for the project. \textcolor{green}{Val to provide expansion.}
  \item[Week 3] \hfill \\
      Voted for a team restructure. \textcolor{green}{Val to provide expansion.}
  \item[Week 4] \hfill \\
      Acquired new team member. \textcolor{green}{Val to provide expansion.}
  \item[Week 5] \hfill \\
      Demonstrated cross-compile milestone in lab. \textcolor{green}{Val to provide expansion.}
  \item[Week 6] \hfill \\
      \textcolor{green}{Val to provide expansion.}
  \item[Week 7] \hfill \\
       \textcolor{green}{Val to provide expansion.}
  \item[Week 8] \hfill \\
      Took a trip to Bunnings with Neil to build the servo mount. \textcolor{green}{Val to provide expansion.}
  \item[Week 9] \hfill \\
       \textcolor{green}{Val to provide expansion.}
  \item[Week 10] \hfill \\
      Presented project to lab group (see appendix). Started work on final project portfolio submission.
  \item[Week 11] \hfill \\
      Worked on final project portfolio submission.
  \item[Week 12] \hfill \\
      Continued work on project portfolio submission.

\end{description}


\subsection{"Alfred" Yang Yuan}

\begin{description}

  \item[Week 3] \hfill \\
      Bought a \rpi. Tried to find a great team aiming for a HD in this course.

  \item[Week 4] \hfill \\
      Joined the team. Read the document and the some source of OpenCV. Then bought an USB camera to run Neil's program on \rpi.
  \item[Week 5] \hfill \\
      Demonstrated cross-compile milestone in lab. Tested two different algorithm to improve performance of the face detection. The two algorithms did not increase the performance dramatically. So I put the face detection algorithm on another thread, in order to make the program run faster.
  \item[Week 6] \hfill \\
      Investigated the implementation of PIR motion sensing. Started to code first version for communication protocol between two \rpis. 
  \item[Week 7] \hfill \\
       Implemented serialisation and synchronisation in communication and copy-on-write in message serialisation. Implemented message scheduling, including FIFS and message dispatch.
  \item[Week 8] \hfill \\
       Wrote the software architecture of this project. Tried to connect all the parts in the project with well defined messaging.
  \item[Week 9] \hfill \\
       Wrote the Client part of the project, and connected the Server and Client via 
		  existing message dispatch process. Increased the face detection performance to make it run much more faster on \rpi. Combined Val's recording code with a pipe.
  \item[Week 10] \hfill \\
      Presented project to lab group (see appendix). Started work on final project portfolio submission.
  \item[Week 11] \hfill \\
      Worked on final project portfolio submission. Wrote partial document about what the architecture, and explained why I built the software like this. In addition, added some comments for some programming tricks.
  \item[Week 12] \hfill \\
      Continued work on project portfolio submission. Cleaned the code for the final submission.

\end{description}

\subsection{Neil Ang}

See appendix for daily log of tasks.

\begin{description}

  \item[Week 2] \hfill \\
      Formed team with Val and discussed ideas for the project. Purchased \rpi. Completed cross-compile milestone. Installed Arch linux on \rpi. Started research into computer vision libraries.
  \item[Week 3] \hfill \\
      Purchased a RPi camera board. Installed Raspbian on \rpi. Researched depth sensing on the device. Prototyped face detection code on MBP with OpenCV. Acquired a web camera and tested on the device. Voted for a team restructure.
  \item[Week 4] \hfill \\
      Acquired new team member. Continued experimenting with OpenCV to improve performance running on the device. Started work on project specification.
  \item[Week 5] \hfill \\
      Demonstrated cross-compile milestone in lab. Continued work on project specification. Wrote test program to simulate face detection for Val.
  \item[Week 6] \hfill \\
      Finished the project specification. Ported the PIR motion sensing Python code to C.
  \item[Week 7] \hfill \\
      Researched IPC techniques. Acquired the servos and started work on porting code to C++.
  \item[Week 8] \hfill \\
      Solved issues with servo powers. Finished work on C++ wrapper for servos. Got face detection based movement working. Took a trip to Bunnings with Val to build the servo mount.
  \item[Week 9] \hfill \\
      Debugged issues with the servo wrapper when running on the Pi. Wrote simple daemon for automatically starting the server/client code. Wrote presentation and speech for next milestone.
  \item[Week 10] \hfill \\
      Presented project to lab group (see appendix). Started work on final project portfolio submission.
  \item[Week 11] \hfill \\
      Worked on final project portfolio submission.
  \item[Week 12] \hfill \\
      Continued work on project portfolio submission.

\end{description}


\section{Summary and Conclusions}

\textcolor{red}{TODO: add something profound, warming, agreeable, perhaps talk about the meaning of life etc.}

\textcolor{red}{... As product the project has commercial potential. Unfortunately there are a lot of hurdles to overcome before this would become a reality. ...}


\textcolor{red}{TODO: update references!!!}


\begin{appendices}

\chapter{Gantt Chart}
\includepdf[fitpaper=true,pages=-]{appendix/gantt-chart.pdf}


\chapter{Prototype Demonstration - Speech}

\newpage

Hi, we're team $\langle$sql injection$\rangle$. We are Alfred, Val and Neil.Because the raspberry pi is small and portable, we wanted to build something that would take advantage of this. We also liked the idea of building something that would respond to its surroundings. So we decided on an ambitious project to modernise the phonograph (commonly known as a dictaphone), but combining subject tracking with targeted audio recording equipment.

Here you can see Alfred, demoing the project. As he moves the left/right/up/down the Pi detects his new position and reorients the the shotgun microphone. This particular microphone is designed for targeted audio, and will record sound up to 3 metres while excluding background noise. The intended use of this product would be for recording lectures and tutorials, and producing podcasts. Our original design also included audio-to-text conversion, so we could generate transcripts, but the translation library we were using didn’t work well with Australian or Russian or Chinese accents... so... we dropped that part.

The first learning objective was about design, development and debugging a complex program on the Pi. With three group members with different backgrounds, we had different preferences for building the project. Our approach was to modularise the solution and allow multiple members work on the same thing but in different languages. For example, the servo code was quickly prototyped in python to ensure it functioned correctly. Once it was working, another group member re-wrote the python into C++ code. As most of you would have come up against, compiling on the Pi was slow. So a lot of early development was done on laptops then later tested on the device. There were a few circumstances where code would work on our machines and not the device, but that was due to the differing versions of g++ we were using and not a big issue to fix. A bonus of working this way, was that we had access to IDE debuggers and static analysers. While trying to improve the performance of our code, we ran the program through "Instruments", which detected where the code was the slowest and also picked up on a memory leak. Since our project was about interacting with the physical world (i.e detecting faces, moving motors, sensing motion). We had to run a lot of manual benchmarking. Basically we would tweak the degree of movement in servos, re-run the code and roughly evaluate if it was getting better or worse.Finally, all source code was checked into git, and hosted on GitHub. We heart GitHub. We used it for our source code, project specification and to record our development logs and bibliography.

Leaning objective 2 was about assessing trade-offs in hardware on a constrained system. To make things easier we put decided to split the workload over two Pis. One for face detection and movement, the other for audio recording and processing. For the subject tracking we looked at multiple solutions. We first thought about using depth sensing with something like a MS Kinect sensor, but found out that it was too resource intensive, so we were limited to 2D visual processing, and settled on face detection because of the suitability for the solution. We could also have done colour tracking or background masking.To perform real-time face detection you take a snapshot from the camera, convert it to grayscale, equalise the histogram, then use a trained classifier (in our case Haar-like features) to scan the image for face shapes at different scales, repeat.There is a C++ based open source library called OpenCV, which implements the classifier we wanted to use. It also supports GPU processing, but only NVIDIA GPUs (which the Pi doesn’t have). So to run it on the Pi we had to do the whole thing on CPU. This gave us the challenge of finding ways to optimise the CPU-based face detection so that it would run in real-time and actually detect faces. We took an iterative approach to this.So we started a basic face detection script and ran it. It used 100\% of the CPU and took about a 15 seconds to process each frame. We were using a 720p camera, so the first obvious step was to reduce the frame size so we were processing to less. At 320x240 we able to process a frame every 4-5 seconds and dropped CPU usage down to 70\%. We also added some limits on how it searched, by giving it a minimum face size, and set it to only perform a rough search for the biggest face. We got it at around 50\% CPU and 2-3 seconds per frame. Not bad.Alfred, then had some brilliant ideas on improve the performance further. Original script was creating a matrix of pixels that were copied into RAM and processed, he switched the code to use the pointers from the cameras instead of rebuilding the pixel matrix locally, and processed them on background threads. When the camera moves the frames that are still being processed are dropped, because the position has changed. Also, because of the way we predict the program would be used, we could reduce the pixels to process even further, by remembering the last face position and targeting just that area first. With all the additions we got the face detection and movement happening at an acceptable speed. There are some limitation to the implementation, such as moving too fast for the camera, or recording a profile shot - but we chalk this up to the constraints of using a raspberry pi. We ran the same scripts on a modern day MBP, and the accuracy and speed were at least 4 times better.A face is a complex shape, so we also thought about detecting just an eye or a nose. We ran some  tests, which improved the overall FPS, but the accuracy was a lot less. So full face detection was our best option. 

Also in attempting to improve the overall performance of project modules (eg face detection, real time recording). We investigated modification to process scheduling, however due to the bare bones nature of our system, there was no competition for CPU time.

Learning objective 4 was about team work. This project was particularly hard to work on as a team. Like other teams in this class would have come across, we faced with the obstacle of limited time for such an ambitious project. Two of our members work full-time, and the other studies full-time, so we had to do a lot of work at night in isolation. The obvious solution was to modularise the project into components, and have each member deliver each week. As can be seen in the table, we also tried to reduce dependencies on deliverables, so if one ran over time it wouldn’t impact the next weeks deliverables.But our biggest team issue with the amount of hardware we were using. Besides the Pis, we only had purchased one microphone, one motion sensor, one set of servos etc. So only one team member had physical access to a piece of hardware at a time. To overcome this obstacle, we came up with the idea of writing hardware simulation scripts. For example, we wrote a C++ program that would simulate face detection and output fake coordinates - This meant Val could fine tune the motor movement without having the physical camera or the face detection code complete.Another problem with the amount of hardware, was that when we did meet together, we needed to bring a lot of equipment. The photo on the slide doesn’t really do justice to what it was like to haul 3x laptops, 3x pis, a router, networking cables, servo cables, PIR sensor breadboards, plus tools etc.Finally, as a team we also wanted to do cross-skill development, which made things slower but increased our personal learning. Each members primary role was designed to make use of their best strengths. But we also included cross-overs in the task so we could each try something new. For example, although Val was our expert hardware guy, Neil (who had very limited hardware experience) got to write some of the C interfaces to the hardware, which is something he had never done before. We did similar things with all roles so that we each learned something new.

\chapter{Prototype Demonstration - Handout}
\includepdf[pages=-]{appendix/summary.pdf}



\chapter{Prototype Demonstration - Slides}
\includepdf[fitpaper=true,pages=-]{appendix/presentation.pdf}


\chapter{Neil's Development Log}
\include{appendix/devlog-neil}

\chapter{Val's Development Log}
\include{appendix/devlog-val}

\chapter{Project Specification}
\includepdf[pages=-]{appendix/specification.pdf}



\end{appendices}

\nocite{*}
\printbibliography[heading=bibintoc]


\end{document}